{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(os.path.dirname(__file__), 'data', 'lnn_forecast.log')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# LNN Model Definition (same as training)\n",
    "def get_activation(name):\n",
    "    if name == \"tanh\":\n",
    "        return torch.tanh\n",
    "    elif name == \"relu\":\n",
    "        return torch.nn.functional.relu\n",
    "    elif name == \"leaky_relu\":\n",
    "        return lambda x: torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n",
    "    elif name == \"gelu\":\n",
    "        return torch.nn.functional.gelu\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {name}\")\n",
    "\n",
    "class LiquidTimeStep(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, activation_name):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_in = nn.Linear(input_size, hidden_size)\n",
    "        self.W_h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tau = nn.Parameter(torch.ones(hidden_size) * 0.1)\n",
    "        self.activation = get_activation(activation_name)\n",
    "\n",
    "    def forward(self, x, h, dt=0.1):\n",
    "        dx = self.activation(self.W_in(x) + self.W_h(h))\n",
    "        h_new = h + dt * (dx - h) / self.tau\n",
    "        return h_new\n",
    "\n",
    "class LiquidNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, activation_name, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = nn.ModuleList([\n",
    "            LiquidTimeStep(input_size if i == 0 else hidden_size, hidden_size, activation_name)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        for t in range(seq_len):\n",
    "            h_t = x[:, t, :]\n",
    "            for layer in self.layers:\n",
    "                h = layer(h_t, h)\n",
    "                h_t = h\n",
    "        h = self.bn(h)\n",
    "        h = self.dropout(h)\n",
    "        return self.output_layer(h)\n",
    "\n",
    "# Define base directories for pretrained files\n",
    "base_trained_models_dir = r\"D:\\Masters thesis PV\\Python codes\\PC_2_lnn\\trained_models_spatiotemporal_ghi_multistep\"\n",
    "base_preprocessed_dir = r\"D:\\Masters thesis PV\\Python codes\\PC_2_lnn\\preprocessed_spatiotemporal_ghi\"\n",
    "\n",
    "# Define paths\n",
    "model_save_dir = base_trained_models_dir\n",
    "save_dir = base_preprocessed_dir\n",
    "model_path = os.path.join(model_save_dir, 'lnn_3hour_spatiotemporal_ghi_multistep.pth')\n",
    "scaler_X_path = os.path.join(save_dir, 'scaler_X_spatiotemporal_ghi_multistep.pkl')\n",
    "scaler_y_path = os.path.join(save_dir, 'scaler_y_spatiotemporal_ghi_multistep.pkl')\n",
    "best_params_path = os.path.join(model_save_dir, 'best_params_spatiotemporal_ghi_multistep.pkl')\n",
    "data_info_path = os.path.join(save_dir, 'data_info_spatiotemporal_ghi_multistep.txt')\n",
    "\n",
    "# Load model and scalers\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "prefix_new = 'spatiotemporal_ghi_multistep'\n",
    "\n",
    "# Load best parameters\n",
    "with open(best_params_path, \"rb\") as f:\n",
    "    best_params = pickle.load(f)\n",
    "\n",
    "# Load data info\n",
    "with open(data_info_path, \"r\") as f:\n",
    "    data_info = f.readlines()\n",
    "    timesteps = int(data_info[0].split(\": \")[1])\n",
    "    n_features = int(data_info[1].split(\": \")[1])\n",
    "\n",
    "# Load model with map_location to handle CPU-only machines\n",
    "lnn_model = LiquidNeuralNetwork(\n",
    "    input_size=n_features,\n",
    "    hidden_size=best_params[\"hidden_size\"],\n",
    "    output_size=6,  # Updated to output 6 GHI values\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    activation_name=best_params[\"activation\"],\n",
    "    dropout_rate=best_params[\"dropout_rate\"]\n",
    ").to(device)\n",
    "lnn_model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "lnn_model.eval()\n",
    "\n",
    "# Load scalers\n",
    "with open(scaler_X_path, \"rb\") as f:\n",
    "    scaler_X = pickle.load(f)\n",
    "with open(scaler_y_path, \"rb\") as f:\n",
    "    scaler_y = pickle.load(f)\n",
    "\n",
    "# Database paths\n",
    "weather_db_path = os.path.join(os.path.dirname(__file__), 'data', 'weather_data.db')\n",
    "ghi_db_path = os.path.join(os.path.dirname(__file__), 'data', 'ghi_data.db')\n",
    "forecast_db_path = os.path.join(os.path.dirname(__file__), 'data', 'forecast_data.db')\n",
    "\n",
    "# Connect to databases\n",
    "conn_weather = sqlite3.connect(weather_db_path)\n",
    "conn_ghi = sqlite3.connect(ghi_db_path)\n",
    "conn_forecast = sqlite3.connect(forecast_db_path)\n",
    "cursor_forecast = conn_forecast.cursor()\n",
    "\n",
    "# Drop and recreate the forecast_data table to ensure correct schema\n",
    "cursor_forecast.execute(\"DROP TABLE IF EXISTS forecast_data\")\n",
    "conn_forecast.commit()\n",
    "logger.info(\"Dropped forecast_data table.\")\n",
    "\n",
    "# Create forecast table with explicit REAL type for GHI and add timestep column\n",
    "cursor_forecast.execute('''\n",
    "    CREATE TABLE forecast_data (\n",
    "        timestamp TEXT,\n",
    "        lat REAL,\n",
    "        lon REAL,\n",
    "        GHI REAL,\n",
    "        timestep INTEGER,\n",
    "        UNIQUE(timestamp, lat, lon, timestep)\n",
    "    )\n",
    "''')\n",
    "conn_forecast.commit()\n",
    "logger.info(\"Created forecast_data table with GHI as REAL and timestep column.\")\n",
    "\n",
    "# Verify the table schema\n",
    "cursor_forecast.execute(\"PRAGMA table_info(forecast_data);\")\n",
    "schema = cursor_forecast.fetchall()\n",
    "logger.info(\"Forecast_data table schema: %s\", schema)\n",
    "\n",
    "# Define the Seoul coordinates (2-km grid, 20x20 points)\n",
    "coords = [(lat, lon) for lat in np.linspace(37.4, 37.7, 20) for lon in np.linspace(126.8, 127.2, 20)]\n",
    "locations_df = pd.DataFrame(coords, columns=['lat', 'lon'])\n",
    "locations_df = locations_df.drop_duplicates(subset=['lat', 'lon'])\n",
    "locations_df['nx'] = locations_df.index // 20\n",
    "locations_df['ny'] = locations_df.index % 20\n",
    "logger.info(f\"locations_df has {len(locations_df)} rows, unique lat/lon pairs: {len(locations_df[['lat', 'lon']].drop_duplicates())}\")\n",
    "\n",
    "# Custom solar geometry functions (as used in training)\n",
    "def calculate_zenith_angle(timestamp, latitude, longitude, standard_meridian=135):\n",
    "    \"\"\"Calculate zenith angle, hour angle, and declination for a given timestamp and location.\"\"\"\n",
    "    lat_rad = np.radians(latitude)\n",
    "    day_of_year = timestamp.timetuple().tm_yday\n",
    "    declination = 23.45 * np.sin(np.radians(360 * (284 + day_of_year) / 365))\n",
    "    decl_rad = np.radians(declination)\n",
    "    B = (360 / 365) * (day_of_year - 81)\n",
    "    EOT = 9.87 * np.sin(np.radians(2 * B)) - 7.53 * np.cos(np.radians(B)) - 1.5 * np.sin(np.radians(B))\n",
    "    hour = timestamp.hour + timestamp.minute / 60.0\n",
    "    time_correction = (4 * (longitude - standard_meridian) + EOT) / 60.0\n",
    "    solar_time = hour + time_correction\n",
    "    hour_angle = 15 * (solar_time - 12)\n",
    "    hour_rad = np.radians(hour_angle)\n",
    "    cos_zenith = (np.sin(lat_rad) * np.sin(decl_rad) +\n",
    "                  np.cos(lat_rad) * np.cos(decl_rad) * np.cos(hour_rad))\n",
    "    zenith_angle = np.degrees(np.arccos(np.clip(cos_zenith, -1, 1)))\n",
    "    return zenith_angle, hour_angle, declination\n",
    "\n",
    "def calc_solar_altitude(timestamp, latitude, longitude):\n",
    "    \"\"\"Calculate solar altitude from zenith angle.\"\"\"\n",
    "    zenith_angle, _, _ = calculate_zenith_angle(timestamp, latitude, longitude)\n",
    "    solar_altitude = 90 - zenith_angle\n",
    "    return solar_altitude\n",
    "\n",
    "def calc_solar_azimuth(zenith, hour_angle, declination, latitude):\n",
    "    \"\"\"Calculate solar azimuth using zenith angle, hour angle, declination, and latitude.\"\"\"\n",
    "    zenith_rad = np.radians(zenith)\n",
    "    hour_rad = np.radians(hour_angle)\n",
    "    decl_rad = np.radians(declination)\n",
    "    lat_rad = np.radians(latitude)\n",
    "    sin_az = np.sin(hour_rad) * np.cos(decl_rad) / np.sin(zenith_rad)\n",
    "    cos_az = (np.sin(zenith_rad) * np.sin(lat_rad) - np.sin(decl_rad)) / (np.cos(zenith_rad) * np.cos(lat_rad))\n",
    "    azimuth = np.degrees(np.arctan2(sin_az, cos_az))\n",
    "    azimuth = (azimuth + 360) % 360\n",
    "    return azimuth\n",
    "\n",
    "def compute_time_features(timestamp):\n",
    "    \"\"\"Compute cyclic time features (day of year, hour).\"\"\"\n",
    "    dt = pd.to_datetime(timestamp)\n",
    "    day_of_year = dt.timetuple().tm_yday\n",
    "    hour = dt.hour + dt.minute / 60.0\n",
    "    day_of_year_sin = np.sin(2 * np.pi * day_of_year / 365.0)\n",
    "    day_of_year_cos = np.cos(2 * np.pi * day_of_year / 365.0)\n",
    "    hour_sin = np.sin(2 * np.pi * hour / 24.0)\n",
    "    hour_cos = np.cos(2 * np.pi * hour / 24.0)\n",
    "    return day_of_year_sin, day_of_year_cos, hour_sin, hour_cos\n",
    "\n",
    "def compute_location_features(lat, lon, lat_min, lat_max, lon_min, lon_max):\n",
    "    \"\"\"Compute cyclic location features (lat, lon) with the same scaling as training.\"\"\"\n",
    "    lat_scaled = (lat - lat_min) / (lat_max - lat_min)\n",
    "    lon_scaled = (lon - lon_min) / (lon_max - lon_min)\n",
    "    lat_sin = np.sin(2 * np.pi * lat_scaled)\n",
    "    lat_cos = np.cos(2 * np.pi * lat_scaled)\n",
    "    lon_sin = np.sin(2 * np.pi * lon_scaled)\n",
    "    lon_cos = np.cos(2 * np.pi * lon_scaled)\n",
    "    return lat_sin, lat_cos, lon_sin, lon_cos\n",
    "\n",
    "def compute_kt(ghi, solar_altitude):\n",
    "    \"\"\"Compute clearness index (Kt).\"\"\"\n",
    "    if solar_altitude <= 0:\n",
    "        return 0.0\n",
    "    I0 = 1367  # Solar constant (W/m^2)\n",
    "    extraterrestrial = I0 * np.sin(np.radians(solar_altitude))\n",
    "    if extraterrestrial <= 0:\n",
    "        return 0.0\n",
    "    kt = ghi / extraterrestrial\n",
    "    return np.clip(kt, 0.0, 1.2)\n",
    "\n",
    "def fetch_and_prepare_data(base_time, n_in=8):\n",
    "    \"\"\"Fetch and prepare historical data for LNN input.\"\"\"\n",
    "    start_time = base_time - timedelta(hours=4)\n",
    "    end_time = base_time\n",
    "    timestamps = pd.date_range(start=start_time, end=end_time, freq='30min')\n",
    "    if len(timestamps) != 9:\n",
    "        logger.error(f\"Expected 9 timesteps, got {len(timestamps)}\")\n",
    "        return None\n",
    "\n",
    "    query_weather = \"\"\"\n",
    "        SELECT timestamp, nx, ny, lat, lon, temperature, wind_speed\n",
    "        FROM weather_data\n",
    "        WHERE data_type = 'historical'\n",
    "        AND timestamp BETWEEN ? AND ?\n",
    "        ORDER BY nx, ny, timestamp\n",
    "    \"\"\"\n",
    "    df_weather = pd.read_sql_query(\n",
    "        query_weather,\n",
    "        conn_weather,\n",
    "        params=(start_time.strftime(\"%Y-%m-%d %H:%M:%S\"), end_time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    )\n",
    "    df_weather['timestamp'] = pd.to_datetime(df_weather['timestamp'])\n",
    "    df_weather = df_weather.drop_duplicates(subset=['timestamp', 'nx', 'ny', 'lat', 'lon'])\n",
    "    logger.info(f\"Fetched weather data: {len(df_weather)} rows\")\n",
    "    logger.info(f\"Weather timestamps: {df_weather['timestamp'].unique()}\")\n",
    "\n",
    "    query_ghi = \"\"\"\n",
    "        SELECT timestamp, lat, lon, GHI, DNI, DHI\n",
    "        FROM ghi_data\n",
    "        WHERE timestamp BETWEEN ? AND ?\n",
    "        ORDER BY timestamp, lat, lon\n",
    "    \"\"\"\n",
    "    df_ghi = pd.read_sql_query(\n",
    "        query_ghi,\n",
    "        conn_ghi,\n",
    "        params=(start_time.strftime(\"%Y-%m-%d %H:%M:%S\"), end_time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    )\n",
    "    df_ghi['timestamp'] = pd.to_datetime(df_ghi['timestamp'])\n",
    "    df_ghi = df_ghi.drop_duplicates(subset=['timestamp', 'lat', 'lon'])\n",
    "    logger.info(f\"Fetched GHI data: {len(df_ghi)} rows\")\n",
    "    logger.info(f\"GHI timestamps: {df_ghi['timestamp'].unique()}\")\n",
    "\n",
    "    if df_weather.empty or df_ghi.empty:\n",
    "        logger.error(\"No historical data available in the specified time range.\")\n",
    "        return None\n",
    "\n",
    "    # Find common timestamps\n",
    "    ghi_timestamps = set(df_ghi['timestamp'])\n",
    "    weather_timestamps = set(df_weather['timestamp'])\n",
    "    common_timestamps = ghi_timestamps.intersection(weather_timestamps)\n",
    "    if not common_timestamps:\n",
    "        logger.error(\"No common timestamps between GHI and weather data.\")\n",
    "        return None\n",
    "    logger.info(f\"Common timestamps: {common_timestamps}\")\n",
    "\n",
    "    # Adjust base_time to the latest common timestamp\n",
    "    latest_common_timestamp = max(common_timestamps)\n",
    "    adjusted_base_time = pd.to_datetime(latest_common_timestamp)\n",
    "    time_diff = (base_time - adjusted_base_time).total_seconds() / 3600.0\n",
    "    if time_diff > 2:  # Don't go back more than 2 hours\n",
    "        logger.error(f\"Adjusted base time {adjusted_base_time} is too far in the past (difference: {time_diff} hours).\")\n",
    "        return None\n",
    "    start_time = adjusted_base_time - timedelta(hours=4)\n",
    "    timestamps = pd.date_range(start=start_time, end=adjusted_base_time, freq='30min')\n",
    "    expected_timesteps = len(timestamps)\n",
    "    logger.info(f\"Adjusted base time to {adjusted_base_time}, new time range: {start_time} to {adjusted_base_time}, expected timesteps: {expected_timesteps}\")\n",
    "\n",
    "    # Filter data to the adjusted time range\n",
    "    df_weather = df_weather[df_weather['timestamp'].isin(timestamps)]\n",
    "    df_ghi = df_ghi[df_ghi['timestamp'].isin(timestamps)]\n",
    "    logger.info(f\"Filtered weather data: {len(df_weather)} rows\")\n",
    "    logger.info(f\"Filtered GHI data: {len(df_ghi)} rows\")\n",
    "\n",
    "    if df_weather.empty or df_ghi.empty:\n",
    "        logger.error(\"No data available in the adjusted time range.\")\n",
    "        return None\n",
    "\n",
    "    # Map both GHI and weather coordinates to locations_df\n",
    "    coords_weather = locations_df[['lat', 'lon']].drop_duplicates().values\n",
    "    tree = cKDTree(coords_weather)\n",
    "\n",
    "    # Map GHI data\n",
    "    coords_ghi = df_ghi[['lat', 'lon']].values\n",
    "    _, idx_ghi = tree.query(coords_ghi)\n",
    "    df_ghi['lat_mapped'] = locations_df.iloc[idx_ghi]['lat'].values\n",
    "    df_ghi['lon_mapped'] = locations_df.iloc[idx_ghi]['lon'].values\n",
    "    df_ghi['nx'] = locations_df.iloc[idx_ghi]['nx'].values\n",
    "    df_ghi['ny'] = locations_df.iloc[idx_ghi]['ny'].values\n",
    "    df_ghi = df_ghi.drop_duplicates(subset=['timestamp', 'nx', 'ny'])\n",
    "    logger.info(f\"After mapping, df_ghi has {len(df_ghi)} rows\")\n",
    "    logger.info(f\"Unique nx, ny pairs in df_ghi: {len(df_ghi[['nx', 'ny']].drop_duplicates())}\")\n",
    "\n",
    "    # Map weather data\n",
    "    coords_weather_data = df_weather[['lat', 'lon']].values\n",
    "    _, idx_weather = tree.query(coords_weather_data)\n",
    "    df_weather['lat_mapped'] = locations_df.iloc[idx_weather]['lat'].values\n",
    "    df_weather['lon_mapped'] = locations_df.iloc[idx_weather]['lon'].values\n",
    "    df_weather['nx'] = locations_df.iloc[idx_weather]['nx'].values\n",
    "    df_weather['ny'] = locations_df.iloc[idx_weather]['ny'].values\n",
    "    df_weather = df_weather.drop_duplicates(subset=['timestamp', 'nx', 'ny'])\n",
    "    logger.info(f\"After mapping, df_weather has {len(df_weather)} rows\")\n",
    "    logger.info(f\"Unique nx, ny pairs in df_weather: {len(df_weather[['nx', 'ny']].drop_duplicates())}\")\n",
    "\n",
    "    # Merge GHI and weather data on timestamp, nx, ny\n",
    "    df_merged = pd.merge(\n",
    "        df_ghi,\n",
    "        df_weather,\n",
    "        on=['timestamp', 'nx', 'ny'],\n",
    "        how='inner'\n",
    "    )\n",
    "    df_merged = df_merged.drop_duplicates(subset=['timestamp', 'nx', 'ny'])\n",
    "    logger.info(f\"Merged data: {len(df_merged)} rows\")\n",
    "\n",
    "    if df_merged.empty:\n",
    "        logger.error(\"No matching GHI and weather data after merging.\")\n",
    "        return None\n",
    "\n",
    "    grouped = df_merged.groupby(['nx', 'ny'])\n",
    "    processed_dfs = []\n",
    "    lat_min, lat_max = locations_df['lat'].min(), locations_df['lat'].max()\n",
    "    lon_min, lon_max = locations_df['lon'].min(), locations_df['lon'].max()\n",
    "\n",
    "    for (nx, ny), group in grouped:\n",
    "        group = group.sort_values('timestamp')\n",
    "        lat, lon = group['lat_mapped_x'].iloc[0], group['lon_mapped_x'].iloc[0]\n",
    "\n",
    "        df_pixel = pd.DataFrame({'timestamp': timestamps})\n",
    "        df_pixel = df_pixel.merge(group, on='timestamp', how='left')\n",
    "        actual_timesteps = len(group['timestamp'].unique())\n",
    "        logger.info(f\"Location (nx={nx}, ny={ny}) has {actual_timesteps} unique timesteps, expected {expected_timesteps}\")\n",
    "        if len(df_pixel) != expected_timesteps:\n",
    "            logger.warning(f\"Location (nx={nx}, ny={ny}) has {len(df_pixel)} timesteps after merge, expected {expected_timesteps}.\")\n",
    "            continue\n",
    "        if actual_timesteps < 5:\n",
    "            logger.warning(f\"Location (nx={nx}, ny={ny}) has too few timesteps ({actual_timesteps}) to forecast.\")\n",
    "            continue\n",
    "\n",
    "        df_pixel['GHI'] = df_pixel['GHI'].interpolate(method='linear', limit_direction='both')\n",
    "        df_pixel['DNI'] = df_pixel['DNI'].interpolate(method='linear', limit_direction='both')\n",
    "        df_pixel['DHI'] = df_pixel['DHI'].interpolate(method='linear', limit_direction='both')\n",
    "        df_pixel['temperature'] = df_pixel['temperature'].interpolate(method='linear', limit_direction='both')\n",
    "        df_pixel['lat'] = lat\n",
    "        df_pixel['lon'] = lon\n",
    "        df_pixel['nx'] = nx\n",
    "        df_pixel['ny'] = ny\n",
    "\n",
    "        df_pixel['Zenith_Angle'], df_pixel['HRA'], df_pixel['DEC'] = zip(*df_pixel['timestamp'].apply(\n",
    "            lambda ts: calculate_zenith_angle(ts, lat, lon)\n",
    "        ))\n",
    "        df_pixel['Solar_Altitude'] = df_pixel['timestamp'].apply(lambda ts: calc_solar_altitude(ts, lat, lon))\n",
    "        df_pixel['Solar_Azimuth'] = df_pixel.apply(\n",
    "            lambda row: calc_solar_azimuth(row['Zenith_Angle'], row['HRA'], row['DEC'], lat), axis=1\n",
    "        )\n",
    "\n",
    "        df_pixel['Kt'] = df_pixel.apply(lambda row: compute_kt(row['GHI'], row['Solar_Altitude']), axis=1)\n",
    "\n",
    "        df_pixel[['day_of_year_sin', 'day_of_year_cos', 'hour_sin', 'hour_cos']] = df_pixel['timestamp'].apply(\n",
    "            lambda ts: pd.Series(compute_time_features(ts))\n",
    "        ).values\n",
    "        df_pixel[['lat_sin', 'lat_cos', 'lon_sin', 'lon_cos']] = pd.Series(\n",
    "            compute_location_features(lat, lon, lat_min, lat_max, lon_min, lon_max)\n",
    "        ).values.repeat(len(df_pixel)).reshape(-1, 4)\n",
    "\n",
    "        processed_dfs.append(df_pixel)\n",
    "\n",
    "    if not processed_dfs:\n",
    "        logger.error(\"No locations had sufficient data for forecasting.\")\n",
    "        return None\n",
    "\n",
    "    return pd.concat(processed_dfs, ignore_index=True)\n",
    "\n",
    "def forecast_ghi(df, n_forecast=6):\n",
    "    \"\"\"Forecast GHI using the LNN model for 6 timesteps.\"\"\"\n",
    "    features = [\n",
    "        'GHI', 'DNI', 'DHI', 'temperature', 'lat_sin', 'lat_cos', 'lon_sin', 'lon_cos',\n",
    "        'Kt', 'Solar_Altitude', 'Solar_Azimuth', 'day_of_year_sin', 'day_of_year_cos',\n",
    "        'hour_sin', 'hour_cos'\n",
    "    ]\n",
    "    feature_columns = []\n",
    "    for t in range(-8, 1):  # t-8 to t\n",
    "        suffix = f\"(t-{abs(t)})\" if t < 0 else \"(t)\"\n",
    "        feature_columns.extend([f\"{feat}{suffix}\" for feat in features])\n",
    "\n",
    "    grouped = df.groupby(['nx', 'ny'])\n",
    "    forecast_dfs = []\n",
    "\n",
    "    for (nx, ny), group in grouped:\n",
    "        group = group.sort_values('timestamp')\n",
    "        lat, lon = group['lat'].iloc[0], group['lon'].iloc[0]\n",
    "\n",
    "        # Prepare input data\n",
    "        data_shifted = pd.concat([group[features].shift(i) for i in range(8, -1, -1)], axis=1)\n",
    "        data_shifted.columns = feature_columns\n",
    "        data_shifted['timestamp'] = group['timestamp']\n",
    "        data_shifted['lat'] = lat\n",
    "        data_shifted['lon'] = lon\n",
    "        data_shifted = data_shifted.dropna()\n",
    "\n",
    "        if data_shifted.empty:\n",
    "            logger.warning(f\"No valid data for forecasting at nx={nx}, ny={ny}\")\n",
    "            continue\n",
    "\n",
    "        # Use the last row for forecasting\n",
    "        X = data_shifted[feature_columns].values[-1:]  # Shape: (1, 135)\n",
    "        X_scaled = scaler_X.transform(X)\n",
    "        X_reshaped = X_scaled.reshape(-1, timesteps, n_features)  # Shape: (1, 9, 15)\n",
    "        X_tensor = torch.tensor(X_reshaped, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Forecast GHI for 6 timesteps\n",
    "        with torch.no_grad():\n",
    "            forecast_scaled = lnn_model(X_tensor)  # Shape: (1, 6)\n",
    "        forecast_ghi = np.zeros_like(forecast_scaled.cpu().numpy())  # Shape: (1, 6)\n",
    "        for i in range(n_forecast):\n",
    "            forecast_ghi[:, i] = scaler_y.inverse_transform(forecast_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "        forecast_ghi = forecast_ghi[0]  # Shape: (6,)\n",
    "\n",
    "        # Generate forecast timestamps\n",
    "        last_timestamp = pd.Timestamp(data_shifted['timestamp'].iloc[-1])\n",
    "        forecast_timestamps = [last_timestamp + timedelta(minutes=30 * i) for i in range(1, n_forecast + 1)]\n",
    "\n",
    "        # Create forecast DataFrame\n",
    "        forecast_rows = []\n",
    "        for i, ts in enumerate(forecast_timestamps):\n",
    "            forecast_rows.append({\n",
    "                'timestamp': ts,\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'GHI': float(forecast_ghi[i]),\n",
    "                'timestep': i + 1  # 1 to 6 for t+1 to t+6\n",
    "            })\n",
    "        forecast_df = pd.DataFrame(forecast_rows)\n",
    "        forecast_df['timestamp'] = pd.to_datetime(forecast_df['timestamp'])\n",
    "        forecast_dfs.append(forecast_df)\n",
    "\n",
    "    if not forecast_dfs:\n",
    "        logger.error(\"No forecasts generated.\")\n",
    "        return None\n",
    "\n",
    "    return pd.concat(forecast_dfs, ignore_index=True)\n",
    "\n",
    "def run_forecast_attempt(base_time, n_in, n_forecast):\n",
    "    try:\n",
    "        df = fetch_and_prepare_data(base_time, n_in=n_in)\n",
    "        if df is None:\n",
    "            return None\n",
    "\n",
    "        forecast_df = forecast_ghi(df, n_forecast=n_forecast)\n",
    "        if forecast_df is None:\n",
    "            logger.error(\"Failed to generate forecasts.\")\n",
    "            return None\n",
    "\n",
    "        # Ensure GHI and timestep are appropriate types and prepare forecast rows\n",
    "        forecast_df['GHI'] = forecast_df['GHI'].astype(float)  # Explicitly convert to float\n",
    "        forecast_df['timestep'] = forecast_df['timestep'].astype(int)  # Explicitly convert to int\n",
    "        forecast_rows = [\n",
    "            (\n",
    "                pd.Timestamp(row.timestamp).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                float(row.lat),\n",
    "                float(row.lon),\n",
    "                float(row.GHI),\n",
    "                int(row.timestep)\n",
    "            )\n",
    "            for row in forecast_df[['timestamp', 'lat', 'lon', 'GHI', 'timestep']].to_records(index=False)\n",
    "        ]\n",
    "\n",
    "        # Insert into database\n",
    "        cursor_forecast.executemany(\n",
    "            \"INSERT OR REPLACE INTO forecast_data (timestamp, lat, lon, GHI, timestep) VALUES (?, ?, ?, ?, ?)\",\n",
    "            forecast_rows\n",
    "        )\n",
    "        conn_forecast.commit()\n",
    "        logger.info(f\"Inserted {len(forecast_rows)} forecast records into forecast_data.db\")\n",
    "\n",
    "        # Save to CSV with relative path for portability\n",
    "        forecast_df_to_save = forecast_df.copy()\n",
    "        forecast_df_to_save['timestamp'] = forecast_df_to_save['timestamp'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        csv_path = os.path.join(os.path.dirname(__file__), 'data', 'forecasted_ghi.csv')\n",
    "        with open(csv_path, 'w') as f:\n",
    "            pass\n",
    "        forecast_df_to_save.to_csv(csv_path, index=False)\n",
    "        logger.info(f\"Forecasted GHI data saved to CSV: {csv_path}\")\n",
    "        return forecast_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during forecast for {base_time}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def run_forecast(base_time, n_in=8, n_forecast=6, retries=3, retry_delay=30):\n",
    "    \"\"\"Run a single forecast cycle with retries.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        logger.info(f\"Attempt {attempt + 1}/{retries}: Running forecast for {base_time}\")\n",
    "        result = run_forecast_attempt(base_time, n_in, n_forecast)\n",
    "        if result is not None:\n",
    "            return result\n",
    "        logger.info(f\"Retrying in {retry_delay/60} minutes...\")\n",
    "        time.sleep(retry_delay)\n",
    "    logger.error(f\"Failed to generate forecast after {retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "def main_forecast_loop(n_in=8, n_forecast=6, interval_minutes=30):\n",
    "    \"\"\"Run forecast loop with updates every interval_minutes (30 or 60).\"\"\"\n",
    "    interval = timedelta(minutes=interval_minutes)\n",
    "\n",
    "    while True:\n",
    "        now = datetime.now().replace(second=0, microsecond=0)\n",
    "        logger.info(f\"Checking forecast cycle at {now}\")\n",
    "\n",
    "        # Set base_time to the nearest past 30-minute mark\n",
    "        minutes = (now.minute // 30) * 30\n",
    "        base_time = now.replace(minute=minutes)\n",
    "\n",
    "        # Skip nighttime (16:00 to 03:00 KST)\n",
    "        current_hour = now.hour\n",
    "        if current_hour >= 16 or current_hour < 3:\n",
    "            next_run = (now + timedelta(days=1)).replace(hour=3, minute=0, second=0, microsecond=0)\n",
    "            if current_hour < 3:\n",
    "                next_run = now.replace(hour=3, minute=0, second=0, microsecond=0)\n",
    "            sleep_seconds = (next_run - now).total_seconds()\n",
    "            logger.info(f\"Current time {now} is in restricted window (16:00-03:00). Sleeping for {sleep_seconds / 60:.2f} minutes until {next_run}\")\n",
    "            time.sleep(sleep_seconds)\n",
    "            continue\n",
    "\n",
    "        # Run the forecast immediately for the nearest past 30-minute mark\n",
    "        run_forecast(base_time, n_in, n_forecast, retries=3, retry_delay=300)\n",
    "\n",
    "        # Calculate the next forecast time\n",
    "        next_base_time = base_time + interval\n",
    "        minutes = (next_base_time.minute // 30) * 30\n",
    "        next_base_time = next_base_time.replace(minute=minutes)\n",
    "\n",
    "        # Sleep until the next forecast time\n",
    "        sleep_seconds = (next_base_time - datetime.now()).total_seconds()\n",
    "        if sleep_seconds > 0:\n",
    "            logger.info(f\"Sleeping for {sleep_seconds / 60:.2f} minutes until next forecast at {next_base_time}\")\n",
    "            time.sleep(sleep_seconds)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Run with 30-minute updates\n",
    "        main_forecast_loop(interval_minutes=30)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Forecast loop stopped by user.\")\n",
    "    finally:\n",
    "        conn_weather.close()\n",
    "        conn_ghi.close()\n",
    "        conn_forecast.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "598087b9ed0ea3fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
