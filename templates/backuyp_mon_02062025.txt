lets analyze our task thoroughly from the start and first we will only focus on the app.py and in future steps we will move to another tasks as the below structure contains our detail what we actually want to do:


"
User inputs: locations of a PV site (x and y coordinates), PV type (this will determine the efficiency), and PV area, tilt (slope) and orientation

and based on those inputs our module will derive solar radiation (show the forecasted GHI) data and show PV efficiency and outputs. And the forecasted PV outputs for the next about 3 hours (similar as GHI forecasted) which depends on our solar prediction model calculating the benchmark predictions (performance evaluations).
The SAM algorithm will use these forecasted GTI (global tiltes/total irriadiance – tilts are inserted by the user) values along with forecasted (for the same timestamps as GHI) weather data (temperature and wind speed) from the KMA API (open meteo forecasted_temp.py) to calculate PV performance metrics (e.g., Pdc, Pac, Yf, PR, CF), which will be evaluated using the IEC metrics outlined in the summary "PV Performance Evaluations Based on IEC Standards" (e.g., PR, Yf, CF, EPI, CSER). These IEC metrics will assess both the LNN model’s forecasting accuracy and the SAM algorithm’s PV output predictions, ensuring a comprehensive evaluation of the system. Finally, the allowing users to input parameters (e.g., location, PV type, tilt, orientation) and view historical, current, and forecasted PV outputs, completing the project’s objective of providing a comprehensive PV performance prediction tool for Seoul. Furthermore, the user will have an option which SAM semi empirical equation model us to be used for the calculations. This will include all the four models from SAM as below
1. Simple efficiency model
2. Sandia array performance model (SAPM)
3. Single diode model (five parameter model)
4. CEC Model (Six parameter model)










The folders directories are as below:
### Updated directory:
pv_performance_tool/
│
├── app.py                  # Flask application
├── templates/
│   └── index.html          # Main webpage
├── static/
│   ├── css/
│   │   └── styles.css      # Custom CSS (if needed)
│   └── js/
│       └── scripts.js      # JavaScript for interactivity and plotting
├── data/
│   ├── locations_eng.xlsx  # Seoul locations (English translated version)
│   ├── ghi_data.db         # SQLite database for historical GHI data
│   ├── weather_data.db     # SQLite database for historical and forecasted weather data tables as forecasted_weather_data  and  weather_data (temperature, wind speed) 
│   ├── forecast_data.db         # SQLite database for forecasted GHI values 

│   ├── historical_ghi_decomposed.csv  # Historical GHI data with decomposed DNI, DHI
│   ├── forecasted_ghi_decomposed.csv  # Forecasted GHI data
│   └── preprocessed_lnn/   # Preprocessed data for LNN model
├── scripts/
│   ├── fetch_ghi_data.py   # Script to fetch and process historical GHI data
│   ├── fetch_historical_weather_data.py  # Script to fetch and store historical weather data (temperature, wind speed) from KMA API 
│   ├── forecasted_temp.py  # Script to fetch forecasted  weather data (temperature, wind speed) from KMA API (open meteo api)  
│   └── lnn_forecast.py     # Script to forecast GHI using LNN model
├── pv_models.py            # SAM models and IEC metrics implementation (needed to implement , not implemented yet) 
└── lnn_model.py            # LNN model definition and utilities (not sure for what purpose I had made the code I don’t think so it is needed if already we have lnn_forecast.py )


Through details:
Step 1: Understand the Requirements and Plan the Implementation
Requirements Overview
1.	User Inputs: 
o	Location of the PV site (x, y coordinates, which map to lat/lon in Seoul, allow user to enter coordinates if the points after decimal increased than our points then round off to the nearest ) also allow user to pin a location for pv evaluations.
o	PV type (determines efficiency; user selects from predefined types from the drop down menu).
o	PV area (in m²).
o	Tilt (slope of the PV panel, in degrees).
o	Orientation (azimuth angle of the PV panel, in degrees for different orientations allow user to enter the different orientations also a dd a drop down menu for some predefined orientations for north, east, west, and south).
o	Choice of SAM semi-empirical model (Simple Efficiency, SAPM, Single Diode, CEC).
2.	Outputs: 
o	Forecasted GHI: Show the forecasted Global Horizontal Irradiance (GHI) for the next 3 hours (6 timesteps at 30-minute intervals) using lnn_forecast.py and forecast_data.db.
o	Forecasted Weather Data: Temperature and wind speed for the same timestamps, fetched from forecasted_temp.py and stored in weather_data.db (table forecasted_weather_data).
o	GTI Calculation: Calculate Global Tilted Irradiance (GTI) using the forecasted GHI, user-provided tilt, and orientation.
o	PV Performance Metrics: Use SAM models to calculate PV outputs (Pdc, Pac) and performance metrics (Yf, PR, CF) based on GTI, temperature, and wind speed.
o	IEC Metrics: Evaluate the LNN model's forecasting accuracy and SAM's PV output predictions using IEC metrics (PR, Yf, CF, EPI, CSER).
o	Historical, Current, and Forecasted Outputs: Allow users to view historical, current, and forecasted PV outputs.
o	Plots: As we have 3ins interval data and predicting the GHI for 3 hours ahead so plot on realtime basis for the GHI showing current time and incorporate the future updates as the lnn_forecast.py updates , similar for the pv power output separately, also showing the plots for I and V for real time and forecasted show at 30 mins interval 






"


//////////////////PV_models.py///////////////////////////////

import numpy as np
import pandas as pd
import sqlite3
from datetime import datetime, timedelta
from scipy.constants import k, e
from datetime import timezone

# KST timezone
KST = timezone(timedelta(hours=9))

# Constants
G_STC = 1000  # STC irradiance (W/m^2)
T_STC = 25  # STC temperature (°C)
AM_STC = 1.5  # STC air mass
q = e  # Electron charge (C)
k_b = k  # Boltzmann constant (J/K)
ALBEDO = 0.2  # Ground reflectance for Seoul (urban area)
MAX_ZENITH_ANGLE = 85.0  # Maximum zenith angle for calculations (degrees)
GHI_MIN = 0.0  # Minimum GHI (W/m^2)
GHI_MAX = 2000.0  # Maximum GHI (W/m^2)

# Perez model coefficients for F1 and F2 (from Perez et al., 1990, as used in PVLib)
PEREZ_COEFFS = [
    {"f11": -0.008, "f12": 0.588, "f13": -0.062, "f21": -0.060, "f22": 0.072, "f23": -0.022},
    {"f11": 0.130, "f12": 0.683, "f13": -0.151, "f21": -0.019, "f22": 0.066, "f23": -0.029},
    {"f11": 0.330, "f12": 0.487, "f13": -0.221, "f21": 0.055, "f22": -0.064, "f23": -0.026},
    {"f11": 0.568, "f12": 0.187, "f13": -0.295, "f21": 0.109, "f22": -0.152, "f23": -0.014},
    {"f11": 0.873, "f12": -0.392, "f13": -0.362, "f21": 0.226, "f22": -0.462, "f23": 0.001},
    {"f11": 1.132, "f12": -1.237, "f13": -0.412, "f21": 0.288, "f22": -0.823, "f23": 0.056},
    {"f11": 1.060, "f12": -1.599, "f13": -0.359, "f21": 0.264, "f22": -1.127, "f23": 0.131},
    {"f11": 0.678, "f12": -0.327, "f13": -0.250, "f21": 0.156, "f22": -1.377, "f23": 0.251}
]

# Real PV module parameters (based on typical datasheets)
PV_MODULES = {
    "Monocrystalline": {
        "name": "SunPower SPR-X21-335",
        "Isc_ref": 6.14,
        "Voc_ref": 67.9,
        "Imp_ref": 5.89,
        "Vmp_ref": 56.9,
        "Pmp_ref": 335,
        "alpha_Isc": 0.0005,
        "beta_Voc": -0.0028,
        "gamma_Pmp": -0.0037,
        "cells": 96,
        "NOCT": 45,
        "area_ref": 1.63,
        "u0": 26.91,
        "u1": 6.2
    },
    "Polycrystalline": {
        "name": "Canadian Solar CS6K-280P",
        "Isc_ref": 9.23,
        "Voc_ref": 38.5,
        "Imp_ref": 8.78,
        "Vmp_ref": 31.9,
        "Pmp_ref": 280,
        "alpha_Isc": 0.0006,
        "beta_Voc": -0.0031,
        "gamma_Pmp": -0.0041,
        "cells": 60,
        "NOCT": 45,
        "area_ref": 1.64,
        "u0": 26.91,
        "u1": 6.2
    },
    "Thin-Film": {
        "name": "First Solar FS-4110-2",
        "Isc_ref": 2.73,
        "Voc_ref": 61.4,
        "Imp_ref": 2.45,
        "Vmp_ref": 44.9,
        "Pmp_ref": 110,
        "alpha_Isc": 0.0004,
        "beta_Voc": -0.0027,
        "gamma_Pmp": -0.0030,
        "cells": 1,
        "NOCT": 45,
        "area_ref": 0.72,
        "u0": 23.37,
        "u1": 5.44
    }
}

def calculate_zenith_angle(timestamp, latitude, longitude, standard_meridian=135):
    """Calculate zenith angle for a given timestamp and location."""
    if isinstance(timestamp, np.datetime64):
        timestamp = pd.Timestamp(timestamp).to_pydatetime()
    lat_rad = np.radians(latitude)
    day_of_year = timestamp.timetuple().tm_yday
    declination = 23.45 * np.sin(np.radians(360 * (284 + day_of_year) / 365))
    decl_rad = np.radians(declination)
    B = (360 / 365) * (day_of_year - 81)
    EOT = 9.87 * np.sin(np.radians(2 * B)) - 7.53 * np.cos(np.radians(B)) - 1.5 * np.sin(np.radians(B))
    hour = timestamp.hour + timestamp.minute / 60.0
    time_correction = (4 * (longitude - standard_meridian) + EOT) / 60.0
    solar_time = hour + time_correction
    hour_angle = 15 * (solar_time - 12)
    hour_rad = np.radians(hour_angle)
    cos_zenith = (np.sin(lat_rad) * np.sin(decl_rad) +
                  np.cos(lat_rad) * np.cos(decl_rad) * np.cos(hour_rad))
    zenith_angle = np.degrees(np.arccos(np.clip(cos_zenith, -1, 1)))
    return zenith_angle, hour_angle, declination

def calc_solar_altitude(timestamp, latitude, longitude):
    """Calculate solar altitude from zenith angle."""
    if isinstance(timestamp, np.datetime64):
        timestamp = pd.Timestamp(timestamp).to_pydatetime()
    zenith_angle, _, _ = calculate_zenith_angle(timestamp, latitude, longitude)
    solar_altitude = 90 - zenith_angle
    return solar_altitude

def calc_solar_azimuth(timestamp, latitude, longitude):
    """Calculate solar azimuth."""
    if isinstance(timestamp, np.datetime64):
        timestamp = pd.Timestamp(timestamp).to_pydatetime()
    zenith, hour_angle, declination = calculate_zenith_angle(timestamp, latitude, longitude)
    zenith_rad = np.radians(zenith)
    hour_rad = np.radians(hour_angle)
    decl_rad = np.radians(declination)
    lat_rad = np.radians(latitude)
    sin_az = np.sin(hour_rad) * np.cos(decl_rad) / np.sin(zenith_rad)
    cos_az = (np.sin(zenith_rad) * np.sin(lat_rad) - np.sin(decl_rad)) / (np.cos(zenith_rad) * np.cos(lat_rad))
    azimuth = np.degrees(np.arctan2(sin_az, cos_az))
    azimuth = (azimuth + 180) % 360
    return azimuth

def calculate_air_mass(zenith_angle):
    """Calculate air mass using the Kasten-Young (1989) formula."""
    if zenith_angle >= 90:
        return float('inf')
    zenith_rad = np.radians(zenith_angle)
    return 1 / (np.cos(zenith_rad) + 0.50572 * (96.07995 - zenith_angle) ** -1.6364)

def decompose_ghi_disc(ghi, timestamp, latitude, longitude, standard_meridian=135):
    """Decompose GHI into DNI and DHI using the DISC model."""
    if isinstance(timestamp, np.datetime64):
        timestamp = pd.Timestamp(timestamp).to_pydatetime()
    zenith_angle, _, _ = calculate_zenith_angle(timestamp, latitude, longitude, standard_meridian)
    zenith_angle = min(zenith_angle, MAX_ZENITH_ANGLE)
    ghi = np.clip(ghi, GHI_MIN, GHI_MAX)
    day_of_year = timestamp.timetuple().tm_yday
    C = np.radians(360 * (day_of_year - 1) / 365)
    re = (1.00011 + 0.034221 * np.cos(C) + 0.00128 * np.sin(C) +
          0.000719 * np.cos(2 * C) + 0.000077 * np.sin(2 * C))
    I0 = 1367 * re
    cos_z = np.cos(np.radians(zenith_angle))
    I0h = I0 * cos_z if cos_z > 0 else 0
    m = calculate_air_mass(zenith_angle)
    m = min(m, 10)
    kt = ghi / I0h if I0h > 0 else 0
    kt = np.clip(kt, 0, 1)
    a = 0.4327 - 0.1925 * m + 0.01453 * m ** 2 - 0.0005127 * m ** 3
    b = -0.3913 + 0.2779 * m - 0.02224 * m ** 2 + 0.0008042 * m ** 3
    c = 0.9585 - 0.08529 * m + 0.005923 * m ** 2 - 0.0001978 * m ** 3
    kn = a + b * kt + c * kt ** 2
    kn = np.clip(kn, 0, 1)
    dni = kn * I0 if cos_z > 0 else 0
    dni = max(dni, 0)
    dhi = ghi - dni * cos_z if cos_z > 0 else 0
    dhi = max(dhi, 0)
    return dni, dhi, I0, m

def calculate_angle_of_incidence(timestamp, latitude, longitude, tilt, orientation):
    """Calculate the angle of incidence (AOI) between the sun and the PV panel."""
    if isinstance(timestamp, np.datetime64):
        timestamp = pd.Timestamp(timestamp).to_pydatetime()
    solar_altitude = calc_solar_altitude(timestamp, latitude, longitude)
    solar_azimuth = calc_solar_azimuth(timestamp, latitude, longitude)
    tilt_rad = np.radians(tilt)
    orientation_rad = np.radians(orientation)
    solar_altitude_rad = np.radians(solar_altitude)
    solar_azimuth_rad = np.radians(solar_azimuth)
    cos_aoi = (np.sin(solar_altitude_rad) * np.cos(tilt_rad) +
               np.cos(solar_altitude_rad) * np.sin(tilt_rad) * np.cos(solar_azimuth_rad - orientation_rad))
    cos_aoi = np.clip(cos_aoi, 0, 1)
    aoi = np.degrees(np.arccos(cos_aoi))
    return aoi

def calculate_iam(aoi):
    """Calculate Incidence Angle Modifier (IAM) per IEC 61853-2."""
    b0 = 0.05
    aoi_rad = np.radians(aoi)
    if aoi >= 90:
        return 0
    iam = 1 - b0 * (1 / np.cos(aoi_rad) - 1)
    return max(iam, 0)

def calculate_perez_diffuse(dni, dhi, zenith_angle, aoi, air_mass, I0, tilt):
    """Calculate diffuse irradiance on a tilted surface using the Perez model."""
    Delta = (dhi * air_mass) / I0 if I0 > 0 else 0
    epsilon = ((dhi + dni) / dhi + 5.535e-6 * zenith_angle ** 3) / (1 + 5.535e-6 * zenith_angle ** 3) if dhi > 0 else 1
    epsilon_bins = [1.065, 1.230, 1.500, 1.950, 2.800, 4.500, 6.200, float('inf')]
    bin_idx = 0
    for i, threshold in enumerate(epsilon_bins):
        if epsilon < threshold:
            bin_idx = i
            break
    coeffs = PEREZ_COEFFS[bin_idx]
    F1 = coeffs["f11"] + coeffs["f12"] * Delta + coeffs["f13"] * zenith_angle
    F1 = max(F1, 0)
    F2 = coeffs["f21"] + coeffs["f22"] * Delta + coeffs["f23"] * zenith_angle
    a = max(0, np.cos(np.radians(aoi)))
    b = max(np.cos(np.radians(85)), np.cos(np.radians(zenith_angle)))
    tilt_rad = np.radians(tilt)
    diffuse_tilted = dhi * ((1 - F1) * (1 + np.cos(tilt_rad)) / 2 + F1 * a / b + F2 * np.sin(tilt_rad))
    return max(diffuse_tilted, 0)

def calculate_gti(ghi, timestamps, latitude, longitude, tilt, orientation):
    """Calculate Global Tilted Irradiance (GTI) with direct, diffuse (Perez model), and albedo components."""
    gti_values = []
    for i, ts in enumerate(timestamps):
        zenith_angle, _, _ = calculate_zenith_angle(ts, latitude, longitude)
        solar_altitude = calc_solar_altitude(ts, latitude, longitude)
        solar_altitude_rad = np.radians(solar_altitude)
        if solar_altitude <= 0 or zenith_angle >= MAX_ZENITH_ANGLE:
            gti_values.append(0)
            continue
        dni, dhi, I0, m = decompose_ghi_disc(ghi[i], ts, latitude, longitude)
        aoi = calculate_angle_of_incidence(ts, latitude, longitude, tilt, orientation)
        iam = calculate_iam(aoi)
        direct_tilted = dni * iam * np.cos(np.radians(aoi))
        diffuse_tilted = calculate_perez_diffuse(dni, dhi, zenith_angle, aoi, m, I0, tilt)
        tilt_rad = np.radians(tilt)
        albedo_tilted = ghi[i] * ALBEDO * (1 - np.cos(tilt_rad)) / 2
        gti = direct_tilted + diffuse_tilted + albedo_tilted
        gti = max(gti, 0)
        gti_values.append(gti)
    return np.array(gti_values)

def calculate_cell_temperature(ghi, temperature, wind_speed, u0, u1):
    """Calculate PV cell temperature using the Faiman model (IEC 61724-2)."""
    if not (len(ghi) == len(temperature) == len(wind_speed)):
        raise ValueError("Length of ghi, temperature, and wind_speed arrays must match")
    wind_speed = np.array([1.0 if ws is None else float(ws) for ws in wind_speed])
    t_cell = temperature + ghi / (u0 + u1 * wind_speed)
    return t_cell

def calculate_inverter_efficiency(pdc, pdc_max):
    """Calculate inverter efficiency as a function of load."""
    load_ratio = pdc / pdc_max if pdc_max > 0 else 0
    if load_ratio < 0.1:
        eta = 0.85
    elif load_ratio < 0.3:
        eta = 0.90
    elif load_ratio < 0.7:
        eta = 0.95
    else:
        eta = 0.96
    return eta

def simple_efficiency_model(gti, timestamps, latitude, longitude, pv_type, area, tilt, orientation, temperature, wind_speed):
    """Simple Efficiency Model."""
    module = PV_MODULES[pv_type]
    efficiency = module["Pmp_ref"] / (G_STC * module["area_ref"])
    t_cell = calculate_cell_temperature(gti, temperature, wind_speed, module["u0"], module["u1"])
    temp_diff = t_cell - T_STC
    adjusted_efficiency = efficiency * (1 + module["gamma_Pmp"] * temp_diff)
    adjusted_efficiency = np.clip(adjusted_efficiency, 0, 1)
    pdc = gti * area * adjusted_efficiency
    pac = np.zeros_like(pdc)
    pdc_max = max(pdc) if max(pdc) > 0 else 1
    for i in range(len(pdc)):
        eta = calculate_inverter_efficiency(pdc[i], pdc_max)
        pac[i] = pdc[i] * eta
    I = pdc / module["Vmp_ref"]
    V = module["Vmp_ref"] * np.ones_like(I)
    return gti, pdc, pac, I, V

def single_diode_model(gti, timestamps, latitude, longitude, pv_type, area, tilt, orientation, temperature, wind_speed):
    """Single Diode Model with Newton-Raphson MPP calculation."""
    module = PV_MODULES[pv_type]
    t_cell = calculate_cell_temperature(gti, temperature, wind_speed, module["u0"], module["u1"])
    Isc_ref = module["Isc_ref"]
    Voc_ref = module["Voc_ref"]
    Imp_ref = module["Imp_ref"]
    Vmp_ref = module["Vmp_ref"]
    alpha_Isc = module["alpha_Isc"]
    beta_Voc = module["beta_Voc"]
    n_cells = module["cells"]
    T_ref = T_STC + 273.15
    Vt_ref = k_b * T_ref / q
    n = 1.0
    I0_ref = Isc_ref / (np.exp(Voc_ref / (n * n_cells * Vt_ref)) - 1)
    Rs = (Voc_ref - Vmp_ref) / Imp_ref
    Rsh_ref = Vmp_ref / (Isc_ref - Imp_ref)
    pdc = []
    pac = []
    I_mpp = []
    V_mpp = []
    for i in range(len(gti)):
        G = gti[i]
        T = t_cell[i] + 273.15
        Vt = k_b * T / q
        Isc = Isc_ref * (G / G_STC) * (1 + alpha_Isc * (T - T_ref))
        I0 = I0_ref * (T / T_ref) ** 3 * np.exp((q * Voc_ref / (n * k_b)) * (1 / T_ref - 1 / T))
        Voc = Voc_ref + beta_Voc * (T - T_ref) + n * n_cells * Vt * np.log(G / G_STC) if G > 0 else 0
        Rsh = Rsh_ref * (G_STC / G) if G > 0 else Rsh_ref
        V = Vmp_ref
        I = Imp_ref
        tol = 1e-6
        max_iter = 100
        for _ in range(max_iter):
            I = Isc - I0 * (np.exp((V + I * Rs) / (n * n_cells * Vt)) - 1) - (V + I * Rs) / Rsh
            P = V * I
            dI_dV = -I0 / (n * n_cells * Vt) * np.exp((V + I * Rs) / (n * n_cells * Vt)) * (1 + Rs * dI_dV) - 1 / Rsh
            dI_dV = dI_dV / (1 + Rs * I0 / (n * n_cells * Vt) * np.exp((V + I * Rs) / (n * n_cells * Vt)))
            dP_dV = I + V * dI_dV
            if abs(dP_dV) < tol:
                break
            V -= dP_dV / (dI_dV + I + V * dI_dV ** 2)
        I = Isc - I0 * (np.exp((V + I * Rs) / (n * n_cells * Vt)) - 1) - (V + I * Rs) / Rsh
        pdc_val = V * I
        eta = calculate_inverter_efficiency(pdc_val, pdc_val * 1.1)
        pac_val = pdc_val * eta
        pdc.append(pdc_val)
        pac.append(pac_val)
        I_mpp.append(I)
        V_mpp.append(V)
    return gti, np.array(pdc), np.array(pac), np.array(I_mpp), np.array(V_mpp)

def cec_model(gti, timestamps, latitude, longitude, pv_type, area, tilt, orientation, temperature, wind_speed):
    """CEC Model (Six Parameter Model) with Newton-Raphson MPP calculation."""
    return single_diode_model(gti, timestamps, latitude, longitude, pv_type, area, tilt, orientation, temperature, wind_speed)

def sandia_array_performance_model(gti, timestamps, latitude, longitude, pv_type, area, tilt, orientation, temperature, wind_speed):
    """Sandia Array Performance Model (SAPM) - Simplified."""
    module = PV_MODULES[pv_type]
    t_cell = calculate_cell_temperature(gti, temperature, wind_speed, module["u0"], module["u1"])
    temp_diff = t_cell - T_STC
    efficiency = module["Pmp_ref"] / (G_STC * module["area_ref"])
    adjusted_efficiency = efficiency * (1 + module["gamma_Pmp"] * temp_diff)
    adjusted_efficiency = np.clip(adjusted_efficiency, 0, 1)
    pdc = gti * area * adjusted_efficiency
    pac = np.zeros_like(pdc)
    pdc_max = max(pdc) if max(pdc) > 0 else 1
    for i in range(len(pdc)):
        eta = calculate_inverter_efficiency(pdc[i], pdc_max)
        pac[i] = pdc[i] * eta
    I = pdc / module["Vmp_ref"]
    V = module["Vmp_ref"] * np.ones_like(I)
    return gti, pdc, pac, I, V

def calculate_iec_metrics(pdc_actual, pac_actual, pdc_forecasted, pac_forecasted, ghi_actual, ghi_forecasted, area, pmp_ref, hours=3):
    """Calculate IEC performance metrics with CSER using IEC 61853-3, returning scalar values."""
    pdc_actual = float(pdc_actual[0]) if isinstance(pdc_actual, np.ndarray) else float(pdc_actual)
    pac_actual = float(pac_actual[0]) if isinstance(pac_actual, np.ndarray) else float(pac_actual)
    ghi_actual = float(ghi_actual[0]) if isinstance(ghi_actual, np.ndarray) else float(ghi_actual)
    hours = max(float(hours), 0.5)
    Yr = ghi_actual * hours / 1000
    Yf = pac_actual * hours / (pmp_ref * 1000)
    PR = Yf / Yr if Yr > 0 else 0
    CF = (pac_actual * hours) / (pmp_ref * 1000 * hours)
    EPI = np.mean([p_f / pac_actual if pac_actual > 0 else 0 for p_f in pac_forecasted])
    measured_output = pac_actual * hours
    expected_output = pmp_ref * G_STC * hours / 1000
    CSER = measured_output / expected_output if expected_output > 0 else 0
    return {
        "Yf": Yf,
        "PR": PR,
        "CF": CF,
        "EPI": EPI,
        "CSER": CSER
    }

def pv_performance(ghi, timestamps, latitude, longitude, pv_type, area, tilt, orientation, temperature, wind_speed, model_type="simple"):
    """Calculate PV performance metrics using SAM models."""
    gti = calculate_gti(ghi, timestamps, latitude, longitude, tilt, orientation)
    if model_type == "simple":
        return simple_efficiency_model(gti, timestamps, latitude, longitude, pv_type, area, tilt, orientation, temperature, wind_speed)
    elif model_type == "sapm":
        return sandia_array_performance_model(gti, timestamps, latitude, longitude, pv_type, area, tilt, orientation, temperature, wind_speed)
    elif model_type == "single_diode":
        return single_diode_model(gti, timestamps, latitude, longitude, pv_type, area, tilt, orientation, temperature, wind_speed)
    elif model_type == "cec":
        return cec_model(gti, timestamps, latitude, longitude, pv_type, area, tilt, orientation, temperature, wind_speed)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

def fetch_forecasted_data(latitude, longitude, start_time, end_time, forecast_db_path, weather_db_path):
    """Fetch forecasted GHI and weather data, falling back to the most recent available forecasts if necessary."""
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.FileHandler('pv_performance.log'), logging.StreamHandler()]
    )
    logger = logging.getLogger(__name__)

    conn_forecast = sqlite3.connect(forecast_db_path)
    conn_weather = sqlite3.connect(weather_db_path)

    # Fetch forecast GHI data within the requested time range
    query_ghi = """
        SELECT timestamp, GHI, timestep
        FROM forecast_data
        WHERE lat = ? AND lon = ?
        AND timestamp BETWEEN ? AND ?
        ORDER BY timestamp, timestep
    """
    start_time_str = start_time.strftime("%Y-%m-%d %H:%M:%S")
    end_time_str = end_time.strftime("%Y-%m-%d %H:%M:%S")
    logger.info(f"Fetching forecast GHI data for lat={latitude}, lon={longitude} between {start_time_str} and {end_time_str}.")
    ghi_df = pd.read_sql_query(
        query_ghi,
        conn_forecast,
        params=(latitude, longitude, start_time_str, end_time_str)
    )
    logger.info(f"Forecast GHI data query result: {len(ghi_df)} rows found.")
    ghi_df['timestamp'] = pd.to_datetime(ghi_df['timestamp']).dt.tz_localize(KST)

    # Fallback: Fetch the most recent forecasts before start_time
    if ghi_df.empty:
        logger.warning(f"No forecast data found between {start_time_str} and {end_time_str}. Attempting fallback query.")
        query_ghi_fallback = """
            SELECT timestamp, GHI, timestep
            FROM forecast_data
            WHERE lat = ? AND lon = ?
            AND timestamp <= ?
            ORDER BY timestamp DESC
            LIMIT 6
        """
        ghi_df = pd.read_sql_query(
            query_ghi_fallback,
            conn_forecast,
            params=(latitude, longitude, start_time_str)
        )
        logger.info(f"Fallback query result: {len(ghi_df)} rows found.")
        ghi_df['timestamp'] = pd.to_datetime(ghi_df['timestamp']).dt.tz_localize(KST)
        if ghi_df.empty:
            logger.error("No forecast data available even with fallback query.")
            conn_forecast.close()
            conn_weather.close()
            return None, None, None, None, None, None

    # Fetch weather forecast data with tolerance for coordinates and time
    lat_lower = latitude - 0.01
    lat_upper = latitude + 0.01
    lon_lower = longitude - 0.01
    lon_upper = longitude + 0.01
    time_min = (ghi_df['timestamp'].min() - pd.Timedelta(minutes=15)).strftime("%Y-%m-%d %H:%M:%S")
    time_max = (ghi_df['timestamp'].max() + pd.Timedelta(minutes=15)).strftime("%Y-%m-%d %H:%M:%S")

    # Only select necessary columns, excluding the 'timestamp' (creation time) column to avoid conflicts
    query_weather = """
        SELECT lat, lon, temperature, wind_speed, forecast_time
        FROM forecasted_weather_data
        WHERE lat BETWEEN ? AND ? AND lon BETWEEN ? AND ?
        AND forecast_time BETWEEN ? AND ?
        ORDER BY forecast_time DESC
    """
    logger.info(f"Fetching weather data for lat between {lat_lower} and {lat_upper}, lon between {lon_lower} and {lon_upper}, forecast_time between {time_min} and {time_max}.")
    weather_df = pd.read_sql_query(
        query_weather,
        conn_weather,
        params=(lat_lower, lat_upper, lon_lower, lon_upper, time_min, time_max)
    )
    logger.info(f"Weather data query result: {len(weather_df)} rows found.")
    if not weather_df.empty:
        logger.debug(f"Weather data sample: {weather_df.head().to_dict(orient='records')}")
    weather_df['forecast_time'] = pd.to_datetime(weather_df['forecast_time']).dt.tz_localize(KST)

    conn_forecast.close()
    conn_weather.close()

    if weather_df.empty:
        logger.error(f"No weather data found for the specified coordinates and time range. Expected data in forecasted_weather_data.")
        raise ValueError("Forecasted weather data is missing, but it should be available in the database. Please check forecasted_weather_data table.")

    # Rename forecast_time to timestamp for merging
    weather_df = weather_df.rename(columns={'forecast_time': 'timestamp'})

    # Deduplicate weather_df by keeping the most recent entry for each (timestamp, lat, lon)
    weather_df = weather_df.sort_values(by=['timestamp', 'lat', 'lon']).drop_duplicates(subset=['timestamp', 'lat', 'lon'], keep='last')

    # Align timestamps and interpolate missing weather data
    expected_timestamps = pd.date_range(start=start_time, end=end_time, freq='30min', tz=KST)
    weather_df = weather_df[weather_df['timestamp'].isin(expected_timestamps)]
    ghi_df = ghi_df[ghi_df['timestamp'].isin(expected_timestamps)]

    # Create a DataFrame with all expected timestamps
    weather_full_df = pd.DataFrame({'timestamp': expected_timestamps})
    weather_full_df = weather_full_df.merge(weather_df, on='timestamp', how='left')
    weather_full_df['temperature'] = weather_full_df['temperature'].interpolate(method='linear', limit_direction='both')
    weather_full_df['wind_speed'] = weather_full_df['wind_speed'].interpolate(method='linear', limit_direction='both')
    weather_full_df['lat'] = weather_full_df['lat'].fillna(latitude)
    weather_full_df['lon'] = weather_full_df['lon'].fillna(longitude)

    # Replace any remaining None or NaN values with defaults
    weather_full_df['temperature'] = weather_full_df['temperature'].fillna(25.0)
    weather_full_df['wind_speed'] = weather_full_df['wind_speed'].fillna(1.0)

    # Merge with GHI data
    merged_df = pd.DataFrame(columns=['timestamp', 'GHI', 'timestep', 'temperature', 'wind_speed'])
    for idx, ghi_row in ghi_df.iterrows():
        ghi_time = pd.Timestamp(ghi_row['timestamp'])
        weather_row = weather_full_df[weather_full_df['timestamp'] == ghi_time]
        if not weather_row.empty:
            merged_row = {
                'timestamp': ghi_time,
                'GHI': ghi_row['GHI'],
                'timestep': ghi_row['timestep'],
                'temperature': weather_row['temperature'].iloc[0],
                'wind_speed': weather_row['wind_speed'].iloc[0]
            }
            merged_df = pd.concat([merged_df, pd.DataFrame([merged_row])], ignore_index=True)
        else:
            logger.warning(f"No weather data found for {ghi_time}. Using default values.")
            merged_row = {
                'timestamp': ghi_time,
                'GHI': ghi_row['GHI'],
                'timestep': ghi_row['timestep'],
                'temperature': 25.0,
                'wind_speed': 1.0
            }
            merged_df = pd.concat([merged_df, pd.DataFrame([merged_row])], ignore_index=True)

    # Replace None or NaN in GHI with 0
    merged_df['GHI'] = merged_df['GHI'].fillna(0.0)

    logger.info(f"Merged dataframe has {len(merged_df)} rows after joining forecast and weather data.")
    return (merged_df['timestamp'].values,
            merged_df['GHI'].values,
            merged_df['temperature'].values,
            merged_df['wind_speed'].values,
            merged_df['GHI'].values,
            merged_df['GHI'].values)

def fetch_historical_data(latitude, longitude, start_time, end_time, ghi_db_path, weather_db_path):
    """Fetch historical GHI and weather data, with fallback to nearest data."""
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.FileHandler('pv_performance.log'), logging.StreamHandler()]
    )
    logger = logging.getLogger(__name__)

    conn_ghi = sqlite3.connect(ghi_db_path)
    conn_weather = sqlite3.connect(weather_db_path)

    query_ghi = """
        SELECT timestamp, GHI
        FROM ghi_data
        WHERE lat = ? AND lon = ?
        AND timestamp BETWEEN ? AND ?
        ORDER BY timestamp
    """
    try:
        ghi_df = pd.read_sql_query(
            query_ghi,
            conn_ghi,
            params=(latitude, longitude, start_time.strftime("%Y-%m-%d %H:%M:%S"), end_time.strftime("%Y-%m-%d %H:%M:%S"))
        )
        ghi_df['timestamp'] = pd.to_datetime(ghi_df['timestamp']).dt.tz_localize(KST)
    except Exception as e:
        logger.error(f"Error querying ghi_data table: {str(e)}")
        conn_ghi.close()
        conn_weather.close()
        raise

    query_weather = """
        SELECT timestamp, temperature, wind_speed
        FROM weather_data
        WHERE lat = ? AND lon = ?
        AND timestamp BETWEEN ? AND ?
        AND data_type = 'historical'
        ORDER BY timestamp
    """
    try:
        weather_df = pd.read_sql_query(
            query_weather,
            conn_weather,
            params=(latitude, longitude, start_time.strftime("%Y-%m-%d %H:%M:%S"), end_time.strftime("%Y-%m-%d %H:%M:%S"))
        )
        weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp']).dt.tz_localize(KST)
        if weather_df['wind_speed'].isnull().any():
            logger.warning(f"Historical weather data contains None values for wind_speed at lat={latitude}, lon={longitude}.")
    except Exception as e:
        logger.error(f"Error querying weather_data table: {str(e)}")
        conn_ghi.close()
        conn_weather.close()
        raise

    merged_df = pd.merge(ghi_df, weather_df, on='timestamp', how='inner')
    if merged_df.empty:
        logger.warning("No historical data found for exact location and time. Attempting to fetch nearest data.")
        time_min = (start_time - pd.Timedelta(hours=1)).strftime("%Y-%m-%d %H:%M:%S")
        time_max = (end_time + pd.Timedelta(hours=1)).strftime("%Y-%m-%d %H:%M:%S")
        query_ghi = """
            SELECT timestamp, GHI
            FROM ghi_data
            WHERE lat = ? AND lon = ?
            AND timestamp BETWEEN ? AND ?
            ORDER BY ABS(strftime('%s', timestamp) - strftime('%s', ?))
            LIMIT 1
        """
        query_weather = """
            SELECT timestamp, temperature, wind_speed
            FROM weather_data
            WHERE lat = ? AND lon = ?
            AND timestamp BETWEEN ? AND ?
            AND data_type = 'historical'
            ORDER BY ABS(strftime('%s', timestamp) - strftime('%s', ?))
            LIMIT 1
        """
        try:
            ghi_df = pd.read_sql_query(
                query_ghi,
                conn_ghi,
                params=(latitude, longitude, time_min, time_max, start_time.strftime("%Y-%m-%d %H:%M:%S"))
            )
            weather_df = pd.read_sql_query(
                query_weather,
                conn_weather,
                params=(latitude, longitude, time_min, time_max, start_time.strftime("%Y-%m-%d %H:%M:%S"))
            )
            ghi_df['timestamp'] = pd.to_datetime(ghi_df['timestamp']).dt.tz_localize(KST)
            weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp']).dt.tz_localize(KST)
            merged_df = pd.merge(ghi_df, weather_df, on='timestamp', how='inner')
            if merged_df['wind_speed'].isnull().any():
                logger.warning(f"Fallback historical weather data contains None values for wind_speed at lat={latitude}, lon={longitude}.")
        except Exception as e:
            logger.error(f"Error in fallback query for historical data: {str(e)}")
            conn_ghi.close()
            conn_weather.close()
            raise

    conn_ghi.close()
    conn_weather.close()

    if merged_df.empty:
        logger.error("No historical data available even after fallback query.")
        raise ValueError("No historical data available for the specified location and time range.")

    # Replace None or NaN values with defaults
    merged_df['GHI'] = merged_df['GHI'].fillna(0.0)
    merged_df['temperature'] = merged_df['temperature'].fillna(25.0)
    merged_df['wind_speed'] = merged_df['wind_speed'].fillna(1.0)

    return (merged_df['timestamp'].values,
            merged_df['GHI'].values,
            merged_df['temperature'].values,
            merged_df['wind_speed'].values)


///////////////////////////////////////////////.......................app.py......................////////////////////////////////////////////

from flask import Flask, render_template, request
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, timezone
import sys
import os
import sqlite3
import logging
from logging.handlers import RotatingFileHandler
from scipy.spatial import cKDTree

# Add the pv_performance_tool directory to the system path
sys.path.append(r"D:\Masters thesis PV\pv_performance_tool")
from pv_models import pv_performance, calculate_iec_metrics, fetch_forecasted_data, fetch_historical_data, PV_MODULES

app = Flask(__name__)

# Configure logging
if not app.debug:
    handler = RotatingFileHandler('pv_performance.log', maxBytes=10000, backupCount=1)
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    app.logger.addHandler(handler)

# Database paths
forecast_db_path = r"D:\Masters thesis PV\pv_performance_tool\data\forecast_data.db"
weather_db_path = r"D:\Masters thesis PV\pv_performance_tool\data\weather_data.db"
ghi_db_path = r"D:\Masters thesis PV\pv_performance_tool\data\ghi_data.db"
locations_path = r"D:\Masters thesis PV\pv_performance_tool\data\locations_eng.xlsx"

# KST timezone
KST = timezone(timedelta(hours=9))

# Define 20x20 grid for Seoul
coords = [(lat, lon) for lat in np.linspace(37.4, 37.7, 20) for lon in np.linspace(126.8, 127.2, 20)]
locations_df = pd.DataFrame(coords, columns=['lat', 'lon'])

def round_to_grid(lat, lon):
    """Round coordinates to the nearest grid point."""
    coords = locations_df[['lat', 'lon']].values
    tree = cKDTree(coords)
    _, idx = tree.query([lat, lon])
    return locations_df.iloc[idx][['lat', 'lon']].values

def fetch_nearest_historical_data(lat, lon, target_time, ghi_db_path, weather_db_path, time_window_hours=1):
    """Fetch the nearest historical data within a time window or from the nearest location."""
    conn_ghi = sqlite3.connect(ghi_db_path)
    conn_weather = sqlite3.connect(weather_db_path)

    # Try fetching data within ±time_window_hours for the exact location
    start_time = target_time - timedelta(hours=time_window_hours)
    end_time = target_time + timedelta(hours=time_window_hours)
    query_ghi = """
        SELECT timestamp, GHI
        FROM ghi_data
        WHERE lat = ? AND lon = ?
        AND timestamp BETWEEN ? AND ?
        ORDER BY ABS(strftime('%s', timestamp) - strftime('%s', ?))
        LIMIT 1
    """
    query_weather = """
        SELECT timestamp, temperature, wind_speed
        FROM weather_data
        WHERE lat = ? AND lon = ?
        AND timestamp BETWEEN ? AND ?
        AND data_type = 'historical'
        ORDER BY ABS(strftime('%s', timestamp) - strftime('%s', ?))
        LIMIT 1
    """
    try:
        ghi_df = pd.read_sql_query(
            query_ghi,
            conn_ghi,
            params=(lat, lon, start_time.strftime("%Y-%m-%d %H:%M:%S"), end_time.strftime("%Y-%m-%d %H:%M:%S"), target_time.strftime("%Y-%m-%d %H:%M:%S"))
        )
        weather_df = pd.read_sql_query(
            query_weather,
            conn_weather,
            params=(lat, lon, start_time.strftime("%Y-%m-%d %H:%M:%S"), end_time.strftime("%Y-%m-%d %H:%M:%S"), target_time.strftime("%Y-%m-%d %H:%M:%S"))
        )
        if not ghi_df.empty and not weather_df.empty:
            merged_df = pd.merge(ghi_df, weather_df, on='timestamp', how='inner')
            if not merged_df.empty:
                conn_ghi.close()
                conn_weather.close()
                return (np.array([pd.to_datetime(merged_df['timestamp'].iloc[0])]),
                        np.array([merged_df['GHI'].iloc[0]]),
                        np.array([merged_df['temperature'].iloc[0]]),
                        np.array([merged_df['wind_speed'].iloc[0]]))
    except Exception as e:
        app.logger.error(f"Error fetching nearest historical data by time: {str(e)}")

    # Fallback: Fetch data for the nearest location within the last 24 hours
    start_time = target_time - timedelta(hours=24)
    end_time = target_time
    query_ghi = """
        SELECT timestamp, lat, lon, GHI
        FROM ghi_data
        WHERE timestamp BETWEEN ? AND ?
        ORDER BY timestamp DESC
    """
    query_weather = """
        SELECT timestamp, lat, lon, temperature, wind_speed
        FROM weather_data
        WHERE timestamp BETWEEN ? AND ?
        AND data_type = 'historical'
        ORDER BY timestamp DESC
    """
    try:
        ghi_df = pd.read_sql_query(
            query_ghi,
            conn_ghi,
            params=(start_time.strftime("%Y-%m-%d %H:%M:%S"), end_time.strftime("%Y-%m-%d %H:%M:%S"))
        )
        weather_df = pd.read_sql_query(
            query_weather,
            conn_weather,
            params=(start_time.strftime("%Y-%m-%d %H:%M:%S"), end_time.strftime("%Y-%m-%d %H:%M:%S"))
        )
        if not ghi_df.empty and not weather_df.empty:
            coords = locations_df[['lat', 'lon']].values
            tree = cKDTree(coords)
            _, idx = tree.query([lat, lon])
            nearest_lat, nearest_lon = locations_df.iloc[idx][['lat', 'lon']].values
            ghi_df = ghi_df[(ghi_df['lat'] == nearest_lat) & (ghi_df['lon'] == nearest_lon)]
            weather_df = weather_df[(weather_df['lat'] == nearest_lat) & (weather_df['lon'] == nearest_lon)]
            merged_df = pd.merge(ghi_df, weather_df, on=['timestamp', 'lat', 'lon'], how='inner')
            if not merged_df.empty:
                conn_ghi.close()
                conn_weather.close()
                return (np.array([pd.to_datetime(merged_df['timestamp'].iloc[0])]),
                        np.array([merged_df['GHI'].iloc[0]]),
                        np.array([merged_df['temperature'].iloc[0]]),
                        np.array([merged_df['wind_speed'].iloc[0]]))
    except Exception as e:
        app.logger.error(f"Error fetching nearest historical data by location: {str(e)}")

    conn_ghi.close()
    conn_weather.close()
    return None, None, None, None

@app.route('/', methods=['GET', 'POST'])
def index():
    # Load locations from Excel file
    try:
        locations_df_excel = pd.read_excel(locations_path)
        cols = locations_df_excel.columns.str.lower().str.strip()
        app.logger.info(f"Columns in locations_eng.xlsx: {list(cols)}")
        lat_col = next(col for col in cols if 'lat' in col or 'y' in col)
        lon_col = next(col for col in cols if 'lon' in col or 'x' in col)
        step2_col = next(col for col in cols if 'step 2' in col)
        step3_col = next(col for col in cols if 'step 3' in col)
        locations_df_excel = locations_df_excel.rename(columns={
            locations_df_excel.columns[cols.tolist().index(lat_col)]: 'lat',
            locations_df_excel.columns[cols.tolist().index(lon_col)]: 'lon'
        })
        location_options = [(row['lat'], row['lon']) for _, row in locations_df_excel.iterrows()]
        location_options_with_labels = [
            (
                row['lat'],
                row['lon'],
                f"{row[locations_df_excel.columns[cols.tolist().index(step2_col)]]}{' - ' + row[locations_df_excel.columns[cols.tolist().index(step3_col)]] if pd.notna(row[locations_df_excel.columns[cols.tolist().index(step3_col)]]) else ''} (Lat: {row['lat']}, Lon: {row['lon']})"
            )
            for _, row in locations_df_excel.iterrows()
        ]
        if not location_options:
            app.logger.error("No locations found in locations_eng.xlsx.")
            return render_template('index.html', error="No locations available. Please check locations_eng.xlsx.",
                                   pv_types=list(PV_MODULES.keys()),
                                   model_types=["simple", "sapm", "single_diode", "cec"],
                                   orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])
        app.logger.info(f"Loaded {len(location_options)} locations from locations_eng.xlsx.")
    except Exception as e:
        app.logger.error(f"Error loading locations_eng.xlsx: {str(e)}")
        location_options = []
        location_options_with_labels = []
        return render_template('index.html', error=f"Error loading locations_eng.xlsx: {str(e)}.",
                               pv_types=list(PV_MODULES.keys()),
                               model_types=["simple", "sapm", "single_diode", "cec"],
                               orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])

    current_results = None
    forecast_results = None
    current_timestamp = ""
    current_ghi = 0
    current_gti = 0
    current_pac = 0
    current_I = 0
    current_V = 0
    timestamps_list = []
    ghi_list = []
    gti_list = []
    pac_list = []
    I_list = []
    V_list = []
    iec_metrics = {}
    forecast_message = ""

    if request.method == 'POST':
        app.logger.info("Received POST request to calculate PV performance.")
        lat_source = request.form.get('lat_source', 'map')
        if lat_source == 'manual':
            try:
                lat = float(request.form['manual_lat'])
                lon = float(request.form['manual_lon'])
                if not (37.4 <= lat <= 37.7 and 126.8 <= lon <= 127.2):
                    app.logger.warning(f"Invalid coordinates: Lat={lat}, Lon={lon}. Out of Seoul bounds.")
                    return render_template('index.html', error="Coordinates out of Seoul bounds (Lat: 37.4-37.7, Lon: 126.8-127.2).",
                                           location_options=location_options, location_options_with_labels=location_options_with_labels,
                                           pv_types=list(PV_MODULES.keys()),
                                           model_types=["simple", "sapm", "single_diode", "cec"],
                                           orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])
                lat, lon = round_to_grid(lat, lon)
                app.logger.info(f"Rounded manual coordinates to: Lat={lat}, Lon={lon}")
            except ValueError:
                app.logger.error("Invalid latitude or longitude values in manual input.")
                return render_template('index.html', error="Invalid latitude or longitude values. Please enter valid numbers.",
                                       location_options=location_options, location_options_with_labels=location_options_with_labels,
                                       pv_types=list(PV_MODULES.keys()),
                                       model_types=["simple", "sapm", "single_diode", "cec"],
                                       orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])
        else:
            try:
                lat = float(request.form['latitude'])
                lon = float(request.form['longitude'])
                if not (37.4 <= lat <= 37.7 and 126.8 <= lon <= 127.2):
                    app.logger.warning(f"Invalid map-selected coordinates: Lat={lat}, Lon={lon}. Out of Seoul bounds.")
                    return render_template('index.html', error="Selected location is outside Seoul bounds (Lat: 37.4-37.7, Lon: 126.8-127.2).",
                                           location_options=location_options, location_options_with_labels=location_options_with_labels,
                                           pv_types=list(PV_MODULES.keys()),
                                           model_types=["simple", "sapm", "single_diode", "cec"],
                                           orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])
                lat, lon = round_to_grid(lat, lon)
                app.logger.info(f"Rounded map-selected coordinates to: Lat={lat}, Lon={lon}")
            except ValueError:
                app.logger.error("Invalid latitude or longitude values from map selection.")
                return render_template('index.html', error="Please select a valid location from the map or search.",
                                       location_options=location_options, location_options_with_labels=location_options_with_labels,
                                       pv_types=list(PV_MODULES.keys()),
                                       model_types=["simple", "sapm", "single_diode", "cec"],
                                       orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])

        pv_type = request.form['pv_type']
        if pv_type not in PV_MODULES:
            app.logger.error(f"Invalid PV type: {pv_type}")
            return render_template('index.html', error="Invalid PV type selected.",
                                   location_options=location_options, location_options_with_labels=location_options_with_labels,
                                   pv_types=list(PV_MODULES.keys()),
                                   model_types=["simple", "sapm", "single_diode", "cec"],
                                   orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])

        try:
            area = float(request.form['area'])
            if area <= 0:
                raise ValueError
        except ValueError:
            app.logger.error("Invalid PV area value.")
            return render_template('index.html', error="PV area must be a positive number.",
                                   location_options=location_options, location_options_with_labels=location_options_with_labels,
                                   pv_types=list(PV_MODULES.keys()),
                                   model_types=["simple", "sapm", "single_diode", "cec"],
                                   orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])

        try:
            tilt = float(request.form['tilt'])
            if tilt < 0 or tilt > 90:
                raise ValueError
        except ValueError:
            app.logger.error("Invalid tilt value.")
            return render_template('index.html', error="Tilt must be between 0 and 90 degrees.",
                                   location_options=location_options, location_options_with_labels=location_options_with_labels,
                                   pv_types=list(PV_MODULES.keys()),
                                   model_types=["simple", "sapm", "single_diode", "cec"],
                                   orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])

        orientation_source = request.form.get('orientation_source', 'dropdown')
        if orientation_source == 'manual':
            try:
                orientation = float(request.form['manual_orientation'])
                orientation = orientation % 360
            except ValueError:
                app.logger.error("Invalid manual orientation value.")
                return render_template('index.html', error="Orientation must be a valid number.",
                                       location_options=location_options, location_options_with_labels=location_options_with_labels,
                                       pv_types=list(PV_MODULES.keys()),
                                       model_types=["simple", "sapm", "single_diode", "cec"],
                                       orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])
        else:
            try:
                orientation = float(request.form['orientation'])
                valid_orientations = [0, -90, 90, 180]
                if orientation not in valid_orientations:
                    app.logger.warning(f"Invalid dropdown orientation: {orientation}")
                    return render_template('index.html', error="Invalid orientation selected.",
                                           location_options=location_options, location_options_with_labels=location_options_with_labels,
                                           pv_types=list(PV_MODULES.keys()),
                                           model_types=["simple", "sapm", "single_diode", "cec"],
                                           orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])
            except ValueError:
                app.logger.error("Invalid orientation value from dropdown.")
                return render_template('index.html', error="Please select a valid orientation from the dropdown.",
                                       location_options=location_options, location_options_with_labels=location_options_with_labels,
                                       pv_types=list(PV_MODULES.keys()),
                                       model_types=["simple", "sapm", "single_diode", "cec"],
                                       orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])

        model_type = request.form['model_type']
        valid_models = ["simple", "sapm", "single_diode", "cec"]
        if model_type not in valid_models:
            app.logger.error(f"Invalid SAM model type: {model_type}")
            return render_template('index.html', error="Invalid SAM model selected.",
                                   location_options=location_options, location_options_with_labels=location_options_with_labels,
                                   pv_types=list(PV_MODULES.keys()),
                                   model_types=["simple", "sapm", "single_diode", "cec"],
                                   orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])

        # Fetch the most recent historical data available (within the last 24 hours)
        now = datetime.now(KST).replace(second=0, microsecond=0)
        current_time = now.replace(minute=(now.minute // 30) * 30)  # Round to nearest 30-minute interval
        historical_start_fetch = current_time - timedelta(hours=24)
        historical_end_fetch = current_time

        app.logger.info(f"Fetching historical data for lat={lat}, lon={lon}, from {historical_start_fetch} to {historical_end_fetch}.")
        try:
            hist_timestamps, hist_ghi, hist_temp, hist_wind = fetch_historical_data(
                lat, lon, historical_start_fetch, historical_end_fetch, ghi_db_path, weather_db_path
            )
        except Exception as e:
            app.logger.error(f"Failed to fetch historical data: {str(e)}")
            hist_timestamps, hist_ghi, hist_temp, hist_wind = None, None, None, None

        if hist_timestamps is None or len(hist_timestamps) == 0:
            app.logger.warning("No historical data available for the selected location within the last 24 hours. Attempting to fetch nearest data.")
            hist_timestamps, hist_ghi, hist_temp, hist_wind = fetch_nearest_historical_data(
                lat, lon, current_time, ghi_db_path, weather_db_path, time_window_hours=1
            )
            if hist_timestamps is None or len(hist_timestamps) == 0:
                app.logger.error("No historical data available even after attempting to fetch nearest data.")
                return render_template('index.html', error="No historical data available for the selected location within the last 24 hours. Please try a different location or ensure historical data is populated.",
                                       location_options=location_options, location_options_with_labels=location_options_with_labels,
                                       pv_types=list(PV_MODULES.keys()),
                                       model_types=["simple", "sapm", "single_diode", "cec"],
                                       orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])

        # Use the most recent timestamp for "current" data
        latest_idx = np.argmax(hist_timestamps)
        current_timestamp = pd.Timestamp(hist_timestamps[latest_idx])
        current_ghi = hist_ghi[latest_idx]
        current_temp = hist_temp[latest_idx]
        current_wind = hist_wind[latest_idx]

        # Calculate current PV performance
        app.logger.info("Calculating current PV performance.")
        try:
            current_gti, current_pdc, current_pac, current_I, current_V = pv_performance(
                np.array([current_ghi]), np.array([current_timestamp]), lat, lon, pv_type, area, tilt, orientation,
                np.array([current_temp]), np.array([current_wind]), model_type
            )
        except Exception as e:
            app.logger.error(f"Error calculating current PV performance: {str(e)}")
            return render_template('index.html', error=f"Error calculating current PV performance: {str(e)}.",
                                   location_options=location_options, location_options_with_labels=location_options_with_labels,
                                   pv_types=list(PV_MODULES.keys()),
                                   model_types=["simple", "sapm", "single_diode", "cec"],
                                   orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])

        # Fetch forecasted data starting from the next 30-minute interval after current_time
        start_time = current_time + timedelta(minutes=30)
        desired_end_time = start_time + timedelta(hours=3)

        app.logger.info(f"Fetching forecasted data for lat={lat}, lon={lon}, starting from {start_time} up to {desired_end_time}.")
        try:
            timestamps, ghi, temperature, wind_speed, ghi_actual, ghi_forecast = fetch_forecasted_data(
                lat, lon, start_time, desired_end_time, forecast_db_path, weather_db_path
            )
        except Exception as e:
            app.logger.error(f"Error fetching forecasted data: {str(e)}")
            error_msg = str(e)
            if "cannot reindex on an axis with duplicate labels" in error_msg:
                error_msg = "Duplicate forecast data entries detected in the database. Please ensure the forecast scripts (e.g., forecasted_temp.py) are running correctly and not inserting duplicate timestamps for the same location."
            return render_template('index.html', error=f"Error fetching forecasted data: {error_msg}. Please ensure the forecast scripts are running and check the database for duplicate entries.",
                                   location_options=location_options, location_options_with_labels=location_options_with_labels,
                                   pv_types=list(PV_MODULES.keys()),
                                   model_types=["simple", "sapm", "single_diode", "cec"],
                                   orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])

        if timestamps is None or len(timestamps) == 0:
            app.logger.error("No forecast data available for the selected location and time range.")
            return render_template('index.html', error="No forecast data available for the selected location and time range. Please ensure the forecast scripts (e.g., forecasted_temp.py, lnn_forecast.py) are running and have populated the database.",
                                   location_options=location_options, location_options_with_labels=location_options_with_labels,
                                   pv_types=list(PV_MODULES.keys()),
                                   model_types=["simple", "sapm", "single_diode", "cec"],
                                   orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])
        else:
            actual_start = pd.Timestamp(timestamps[0])
            actual_end = pd.Timestamp(timestamps[-1])
            # Convert numpy.timedelta64 to seconds and then to hours
            delta = actual_end - actual_start
            forecast_period_seconds = delta / np.timedelta64(1, 's')
            forecast_period_hours = forecast_period_seconds / 3600.0
            if forecast_period_hours < 3:
                forecast_message = f"Forecast data available only from {actual_start.strftime('%Y-%m-%d %H:%M:%S')} to {actual_end.strftime('%Y-%m-%d %H:%M:%S')} ({forecast_period_hours:.1f} hours). Forecasts beyond 6 PM may not be available."
            else:
                forecast_message = f"Forecast data available from {actual_start.strftime('%Y-%m-%d %H:%M:%S')} to {actual_end.strftime('%Y-%m-%d %H:%M:%S')} (3 hours)."

        # Calculate forecasted PV performance
        app.logger.info("Calculating forecasted PV performance with available data.")
        try:
            gti, pdc, pac, I, V = pv_performance(
                ghi, timestamps, lat, lon, pv_type, area, tilt, orientation, temperature, wind_speed, model_type
            )
        except Exception as e:
            app.logger.error(f"Error calculating forecasted PV performance: {str(e)}")
            return render_template('index.html', error=f"Error calculating forecasted PV performance: {str(e)}.",
                                   location_options=location_options, location_options_with_labels=location_options_with_labels,
                                   pv_types=list(PV_MODULES.keys()),
                                   model_types=["simple", "sapm", "single_diode", "cec"],
                                   orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])

        # Calculate IEC metrics
        app.logger.info("Calculating IEC metrics.")
        # Convert numpy.timedelta64 to seconds and then to hours
        delta = timestamps[-1] - timestamps[0]
        actual_forecast_seconds = delta / np.timedelta64(1, 's')
        actual_forecast_hours = max(actual_forecast_seconds / 3600.0, 0.5)
        iec_metrics = calculate_iec_metrics(
            pdc_actual=current_pdc, pac_actual=current_pac,
            pdc_forecasted=pdc, pac_forecasted=pac,
            ghi_actual=np.array([current_ghi]), ghi_forecasted=ghi,
            area=area, pmp_ref=PV_MODULES[pv_type]["Pmp_ref"],
            hours=actual_forecast_hours
        )

        # Prepare current data for plotting with default values for None
        current_timestamp_str = current_timestamp.strftime("%Y-%m-%d %H:%M:%S")
        current_ghi = float(current_ghi if current_ghi is not None else 0.0)
        current_gti = float(current_gti[0] if current_gti[0] is not None else 0.0)
        current_pdc = float(current_pdc[0] if current_pdc[0] is not None else 0.0)
        current_pac = float(current_pac[0] if current_pac[0] is not None else 0.0)
        current_I = float(current_I[0] if current_I[0] is not None else 0.0)
        current_V = float(current_V[0] if current_V[0] is not None else 0.0)
        current_temp = float(current_temp if current_temp is not None else 25.0)
        current_wind = float(current_wind if current_wind is not None else 1.0)

        current_results = {
            'timestamp': current_timestamp_str,
            'ghi': round(current_ghi, 2),
            'gti': round(current_gti, 2),
            'pdc': round(current_pdc, 2),
            'pac': round(current_pac, 2),
            'I': round(current_I, 2),
            'V': round(current_V, 2),
            'temperature': round(current_temp, 2),
            'wind_speed': round(current_wind, 2)
        }

        # Prepare forecasted data for display and plotting with default values for None
        forecast_results = []
        timestamps_list = []
        ghi_list = []
        gti_list = []
        pac_list = []
        I_list = []
        V_list = []
        for i in range(len(timestamps)):
            timestamp = pd.Timestamp(timestamps[i])
            timestamps_list.append(timestamp.strftime("%Y-%m-%d %H:%M:%S"))
            ghi_val = float(ghi[i] if ghi[i] is not None else 0.0)
            gti_val = float(gti[i] if gti[i] is not None else 0.0)
            pdc_val = float(pdc[i] if pdc[i] is not None else 0.0)
            pac_val = float(pac[i] if pac[i] is not None else 0.0)
            I_val = float(I[i] if I[i] is not None else 0.0)
            V_val = float(V[i] if V[i] is not None else 0.0)
            temp_val = float(temperature[i] if temperature[i] is not None else 25.0)
            wind_val = float(wind_speed[i] if wind_speed[i] is not None else 1.0)
            ghi_list.append(round(ghi_val, 2))
            gti_list.append(round(gti_val, 2))
            pac_list.append(round(pac_val, 2))
            I_list.append(round(I_val, 2))
            V_list.append(round(V_val, 2))
            forecast_results.append({
                'timestamp': timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                'ghi': round(ghi_val, 2),
                'gti': round(gti_val, 2),
                'pdc': round(pdc_val, 2),
                'pac': round(pac_val, 2),
                'I': round(I_val, 2),
                'V': round(V_val, 2),
                'temperature': round(temp_val, 2),
                'wind_speed': round(wind_val, 2)
            })

        app.logger.info("Rendering template with current and forecasted results.")
        return render_template('index.html',
                               current_results=current_results,
                               forecast_results=forecast_results,
                               current_timestamp=current_timestamp_str,
                               current_ghi=current_ghi,
                               current_gti=current_gti,
                               current_pac=current_pac,
                               current_I=current_I,
                               current_V=current_V,
                               timestamps=timestamps_list,
                               ghi=ghi_list,
                               gti=gti_list,
                               pac=pac_list,
                               I=I_list,
                               V=V_list,
                               iec_metrics={k: round(v, 3) for k, v in iec_metrics.items()},
                               pv_types=list(PV_MODULES.keys()),
                               model_types=["simple", "sapm", "single_diode", "cec"],
                               location_options=location_options,
                               location_options_with_labels=location_options_with_labels,
                               orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")],
                               selected_lat=lat,
                               selected_lon=lon,
                               selected_pv_type=pv_type,
                               selected_area=area,
                               selected_tilt=tilt,
                               selected_orientation=orientation,
                               selected_model_type=model_type,
                               forecast_message=forecast_message)

    app.logger.info("Rendering initial template for GET request.")
    return render_template('index.html',
                           pv_types=list(PV_MODULES.keys()),
                           model_types=["simple", "sapm", "single_diode", "cec"],
                           location_options=location_options,
                           location_options_with_labels=location_options_with_labels,
                           orientation_options=[(0, "South (0°)"), (-90, "East (-90°)"), (90, "West (90°)"), (180, "North (180°)")])

if __name__ == '__main__':
    app.run(debug=True)


////////////////////////////////////.............................forecasted_temp.py.............................////////////////////////////////////


import requests
import pandas as pd
import sqlite3
from datetime import datetime, timedelta, timezone
import time
import logging
import urllib3
import numpy as np
from scipy.spatial import cKDTree

# Suppress InsecureRequestWarning
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(r"D:\Masters thesis PV\pv_performance_tool\data\forecasted_temp.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Database setup
db_path = r"D:\Masters thesis PV\pv_performance_tool\data\weather_data.db"
conn = sqlite3.connect(db_path)
cursor = conn.cursor()

# Drop and recreate the table with a corrected UNIQUE constraint
cursor.execute("DROP TABLE IF EXISTS forecasted_weather_data")
try:
    cursor.execute('''
        CREATE TABLE forecasted_weather_data (
            timestamp TEXT,
            nx INTEGER,
            ny INTEGER,
            lat REAL,
            lon REAL,
            temperature REAL,
            wind_speed REAL,
            forecast_time TEXT,
            data_type TEXT,
            UNIQUE(nx, ny, forecast_time, data_type)
        )
    ''')
    conn.commit()
    logger.info("Recreated forecasted_weather_data table with corrected UNIQUE constraint.")
except Exception as e:
    logger.error(f"Error setting up database: {str(e)}")
    raise

# Open-Meteo request tracking
open_meteo_requests = 0
OPEN_METEO_DAILY_LIMIT = 9500

def reset_request_counter():
    """Reset the Open-Meteo request counter daily."""
    global open_meteo_requests
    open_meteo_requests = 0
    logger.info("Reset Open-Meteo request counter for the day.")

def get_row_count(table_name, data_type):
    cursor.execute(f"SELECT COUNT(*) FROM {table_name} WHERE data_type = ?", (data_type,))
    return cursor.fetchone()[0]

# Generate 20x20 grid for Seoul (lat: 37.4–37.7, lon: 126.8–127.2)
coords = [(lat, lon) for lat in np.linspace(37.4, 37.7, 20) for lon in np.linspace(126.8, 127.2, 20)]
locations_df = pd.DataFrame(coords, columns=['lat', 'lon'])
locations_df['nx'] = locations_df.index // 20
locations_df['ny'] = locations_df.index % 20

# KST timezone
KST = timezone(timedelta(hours=9))

def fetch_open_meteo_data(locations, start_time, end_time, include_wind_speed=True):
    """Fetch forecasted temperature and wind speed from Open-Meteo."""
    global open_meteo_requests
    all_data_points = []
    for lat, lon in locations:
        if open_meteo_requests >= OPEN_METEO_DAILY_LIMIT:
            logger.warning(f"Approaching Open-Meteo daily limit ({open_meteo_requests}/{OPEN_METEO_DAILY_LIMIT}). Skipping request for lat={lat}, lon={lon}.")
            all_data_points.append((lat, lon, None))
            continue
        url = f"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&hourly=temperature_2m,wind_speed_10m"
        url += f"&start_date={start_time.strftime('%Y-%m-%d')}&end_date={end_time.strftime('%Y-%m-%d')}"
        try:
            response = requests.get(url, timeout=30)
            open_meteo_requests += 1
            if response.status_code == 200:
                data = response.json()
                timestamps = pd.to_datetime(data['hourly']['time']).tz_localize(KST)
                temperatures = data['hourly']['temperature_2m']
                winds = data['hourly']['wind_speed_10m']
                data_points = []
                for ts, temp, wind in zip(timestamps, temperatures, winds):
                    if start_time <= ts <= end_time:
                        if not (-20 <= temp <= 50):
                            logger.warning(f"Invalid temperature value {temp} at lat={lat}, lon={lon}, timestamp={ts}")
                            continue
                        wind_ms = wind / 3.6
                        if not (0 <= wind_ms <= 50):
                            logger.warning(f"Invalid wind speed value {wind_ms} at lat={lat}, lon={lon}, timestamp={ts}")
                            continue
                        data_point = {'forecast_time': ts, 'temperature': temp, 'wind_speed': wind_ms}
                        data_points.append(data_point)
                all_data_points.append((lat, lon, data_points))
                logger.info(f"Fetched Open-Meteo data for lat={lat}, lon={lon} (Request count: {open_meteo_requests})")
            else:
                logger.error(f"Open-Meteo request failed for lat={lat}, lon={lon}: Status {response.status_code}")
                all_data_points.append((lat, lon, None))
        except Exception as e:
            logger.error(f"Open-Meteo error for lat={lat}, lon={lon}: {str(e)}")
            all_data_points.append((lat, lon, None))
        time.sleep(0.1)
    return all_data_points

def interpolate_to_30min(data_points, start_time, end_time):
    """Interpolate hourly forecast data to 30-minute intervals."""
    if not data_points:
        return []
    df = pd.DataFrame(data_points)
    df.set_index('forecast_time', inplace=True)
    if df.index.tz is None:
        df.index = df.index.tz_localize(KST)
    else:
        df.index = df.index.tz_convert(KST)
    time_range = pd.date_range(start=start_time, end=end_time, freq='30min', tz=KST)
    df_30min = df.reindex(time_range, method='nearest').interpolate(method='linear')
    interpolated_data = []
    for dt, row in df_30min.iterrows():
        interpolated_data.append({
            'forecast_time': dt,
            'temperature': row['temperature'],
            'wind_speed': row['wind_speed']
        })
    return interpolated_data

def interpolate_forecast(lat, lon, timestamp, conn, time_window_hours=4):
    """Interpolate missing forecast data using historical data."""
    logger.info(f"Attempting interpolation for lat={lat}, lon={lon}, timestamp={timestamp}")
    if timestamp.tzinfo is None:
        timestamp = timestamp.replace(tzinfo=KST)
    else:
        timestamp = timestamp.astimezone(KST)
    start_time = timestamp - timedelta(hours=time_window_hours)
    end_time = timestamp + timedelta(hours=time_window_hours)
    query = """
        SELECT timestamp, temperature, wind_speed
        FROM weather_data
        WHERE lat = ? AND lon = ? AND data_type = 'historical'
        AND timestamp BETWEEN ? AND ?
        ORDER BY timestamp
    """
    try:
        df = pd.read_sql_query(
            query, conn, params=(lat, lon, start_time.strftime("%Y-%m-%d %H:%M:%S"), end_time.strftime("%Y-%m-%d %H:%M:%S"))
        )
        if len(df) < 2:
            logger.warning(f"Insufficient data for interpolation at lat={lat}, lon={lon}, timestamp={timestamp}")
            return None
        df['timestamp'] = pd.to_datetime(df['timestamp']).dt.tz_localize(KST)
        df.set_index('timestamp', inplace=True)
        target_df = pd.DataFrame(index=[pd.Timestamp(timestamp)])
        combined_df = pd.concat([df, target_df]).sort_index()
        combined_df['temperature'] = combined_df['temperature'].interpolate(method='linear', limit_direction='both')
        combined_df['wind_speed'] = combined_df['wind_speed'].interpolate(method='linear', limit_direction='both')
        result = combined_df.loc[timestamp, ['temperature', 'wind_speed']] if timestamp in combined_df.index else None
        if result is not None and -20 <= result['temperature'] <= 50 and (result['wind_speed'] is None or 0 <= result['wind_speed'] <= 50):
            logger.info(f"Interpolated: temperature={result['temperature']:.2f}°C, wind_speed={result['wind_speed']}m/s for lat={lat}, lon={lon}, timestamp={timestamp}")
            return result.to_dict()
        else:
            logger.warning(f"Invalid interpolated values: {result} at lat={lat}, lon={lon}, timestamp={timestamp}")
            return None
    except Exception as e:
        logger.error(f"Error during interpolation: {str(e)}")
        return None

def fetch_forecasted_data(now):
    total_locations = len(locations_df)
    forecast_batch_size = 25
    start_time_forecast = now.replace(minute=0, second=0, microsecond=0)
    end_time_forecast = start_time_forecast + timedelta(hours=6)
    forecast_timestamps = pd.date_range(start=start_time_forecast, end=end_time_forecast, freq='30min', tz=KST)
    forecast_rows = []
    skipped_timestamps = 0
    logger.info(f"Fetching forecasted data from {start_time_forecast} to {end_time_forecast}")
    logger.info(f"Forecast row count before fetch: {get_row_count('forecasted_weather_data', 'forecasted')}")
    coords_grid = locations_df[['lat', 'lon']].drop_duplicates().values
    tree = cKDTree(coords_grid)
    for batch_start in range(0, total_locations, forecast_batch_size):
        batch_end = min(batch_start + forecast_batch_size, total_locations)
        batch_df = locations_df.iloc[batch_start:batch_end]
        logger.info(f"Processing forecast batch {batch_start // forecast_batch_size + 1}/{(total_locations - 1) // forecast_batch_size + 1} (points {batch_start} to {batch_end - 1})")
        locations_to_fetch = [(row['lat'], row['lon']) for _, row in batch_df.iterrows()]
        om_data = fetch_open_meteo_data(locations_to_fetch, start_time_forecast, end_time_forecast, include_wind_speed=True)
        for (lat, lon, data_points) in om_data:
            _, idx = tree.query([lat, lon])
            mapped_lat, mapped_lon = coords_grid[idx]
            if mapped_lat != lat or mapped_lon != lon:
                logger.info(f"Mapped coordinates ({lat}, {lon}) to grid point ({mapped_lat}, {mapped_lon})")
            lat, lon = mapped_lat, mapped_lon
            nx = locations_df[(locations_df['lat'] == lat) & (locations_df['lon'] == lon)]['nx'].iloc[0]
            ny = locations_df[(locations_df['lat'] == lat) & (locations_df['lon'] == lon)]['ny'].iloc[0]
            if data_points:
                interpolated_data = interpolate_to_30min(data_points, start_time_forecast, end_time_forecast)
                for dp in interpolated_data:
                    forecast_rows.append((
                        now.strftime('%Y-%m-%d %H:%M:%S'),
                        nx, ny, lat, lon,
                        dp['temperature'],
                        dp['wind_speed'],
                        dp['forecast_time'].strftime('%Y-%m-%d %H:%M:%S'),
                        'forecasted'
                    ))
            else:
                for ts in forecast_timestamps:
                    interpolated = interpolate_forecast(lat, lon, ts, conn)
                    if interpolated:
                        forecast_rows.append((
                            now.strftime('%Y-%m-%d %H:%M:%S'),
                            nx, ny, lat, lon,
                            interpolated['temperature'],
                            interpolated['wind_speed'],
                            ts.strftime('%Y-%m-%d %H:%M:%S'),
                            'interpolated'
                        ))
                    else:
                        logger.error(f"No forecast data for lat={lat}, lon={lon}, forecast_time={ts}")
                        skipped_timestamps += 1
        time.sleep(5)

    if forecast_rows:
        try:
            cursor.executemany(
                "INSERT OR REPLACE INTO forecasted_weather_data (timestamp, nx, ny, lat, lon, temperature, wind_speed, forecast_time, data_type) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
                forecast_rows)
            conn.commit()
            logger.info(f"Inserted {len(forecast_rows)} forecasted weather records")
            logger.info(f"Number of skipped timestamps: {skipped_timestamps}")
            logger.info(f"Forecast row count after insert: {get_row_count('forecasted_weather_data', 'forecasted')}")
        except Exception as e:
            logger.error(f"Error inserting forecasted weather data: {str(e)}")

    for _, row in locations_df.iterrows():
        nx, ny = row["nx"], row["ny"]
        cutoff = (now - timedelta(hours=6)).strftime("%Y-%m-%d %H:%M:%S")
        cursor.execute("""
            DELETE FROM forecasted_weather_data
            WHERE nx = ? AND ny = ? AND timestamp < ?
        """, (nx, ny, cutoff))
    conn.commit()
    logger.info(f"Cleaned up forecasted data, keeping data after {cutoff}")

def fetch_and_store_forecasted_temp():
    total_locations = len(locations_df)
    last_reset_date = datetime.now(KST).date()
    now = datetime.now(KST).replace(second=0, microsecond=0)
    current_hour = now.hour
    current_minute = now.minute
    last_forecast_fetch = now.replace(minute=5, second=0, microsecond=0)
    if current_minute < 5:
        last_forecast_fetch -= timedelta(hours=1)
    should_fetch_initial = 6 <= current_hour < 22
    if should_fetch_initial:
        logger.info(f"Performing initial fetch at {now}")
        logger.info(f"Initial forecast fetch for timestamp: {last_forecast_fetch}")
        fetch_forecasted_data(last_forecast_fetch)
    else:
        logger.info(f"Current time {now} is outside operating hours (06:00–22:00 KST). Skipping initial fetch.")
    while True:
        now = datetime.now(KST).replace(second=0, microsecond=0)
        current_hour = now.hour
        current_minute = now.minute
        current_date = now.date()
        if current_date != last_reset_date:
            reset_request_counter()
            last_reset_date = current_date
        if 6 <= current_hour < 22:
            should_fetch_forecast = current_minute == 5
            if should_fetch_forecast:
                fetch_forecasted_data(now)
        next_fetch = now.replace(second=0, microsecond=0)
        if current_minute < 5:
            next_fetch = next_fetch.replace(minute=5)
        else:
            next_fetch = next_fetch.replace(minute=5) + timedelta(hours=1)
        if next_fetch.hour >= 22:
            next_fetch = next_fetch.replace(hour=6, minute=5) + timedelta(days=1)
        elif next_fetch.hour < 6:
            next_fetch = next_fetch.replace(hour=6, minute=5)
        sleep_seconds = (next_fetch - now).total_seconds()
        if sleep_seconds > 0:
            logger.info(f"Sleeping for {sleep_seconds / 60:.2f} minutes until next fetch at {next_fetch}")
            time.sleep(sleep_seconds)
        else:
            logger.info("No sleep needed, proceeding to next iteration immediately.")

if __name__ == "__main__":
    try:
        fetch_and_store_forecasted_temp()
    except KeyboardInterrupt:
        logger.info("Stopped by user.")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
    finally:
        conn.close()
	

///////////////////////////////////////////////........................................fetch_ghi.py........................./////////////////////////////////////////

import requests
import os
import netCDF4 as nc
import numpy as np
from datetime import datetime, timedelta
import pandas as pd
import sqlite3
import pyproj
import time
import logging
from pyproj import Transformer

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(r"D:\Masters thesis PV\pv_performance_tool\data\fetch_ghi_data.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Set up parameters
auth_key = "LoBU7l_-QDuAVO5f_iA7ZQ"
base_url = "https://apihub.kma.go.kr/api/typ05/api/GK2A/LE2/SWRAD/KO/data"
output_dir = r"D:\Masters thesis PV\pv_performance_tool\data\nc_files"
db_path = r"D:\Masters thesis PV\pv_performance_tool\data\ghi_data.db"

# Create directories
os.makedirs(output_dir, exist_ok=True)

# Projection and Seoul region config
proj = "+proj=lcc +lat_0=38 +lon_0=126 +lat_1=30 +lat_2=60 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs"
extent = (-899000, 899000, -899000, 899000)
seoul_bbox = {"min_lon": 126.8, "max_lon": 127.2, "min_lat": 37.4, "max_lat": 37.7}

# Define the 20x20 grid directly from locations_df (consistent with other scripts)
coords = [(lat, lon) for lat in np.linspace(37.4, 37.7, 20) for lon in np.linspace(126.8, 127.2, 20)]
locations_df = pd.DataFrame(coords, columns=['lat', 'lon'])
locations_df['nx'] = locations_df.index // 20
locations_df['ny'] = locations_df.index % 20

# Transformers
wgs84 = pyproj.CRS("EPSG:4326")
lcc = pyproj.CRS(proj)
transformer_to_lcc = Transformer.from_crs(wgs84, lcc, always_xy=True)
transformer_to_wgs84 = Transformer.from_crs(lcc, wgs84, always_xy=True)

# Convert Seoul bbox to LCC
min_x, min_y = transformer_to_lcc.transform(seoul_bbox["min_lon"], seoul_bbox["max_lat"])
max_x, max_y = transformer_to_lcc.transform(seoul_bbox["max_lon"], seoul_bbox["min_lat"])

# Grid config
total_width = total_height = 900
pixel_size_x = (extent[1] - extent[0]) / total_width
pixel_size_y = (extent[3] - extent[2]) / total_height

# Align bounding box to ensure exact 20x20 grid
min_x = np.floor(min_x / pixel_size_x) * pixel_size_x
max_x = min_x + 20 * pixel_size_x
min_y = np.ceil(max_y / pixel_size_y) * pixel_size_y - 20 * pixel_size_y
max_y = min_y + 20 * pixel_size_y

# Pixel indices
col_start = int((min_x - extent[0]) / pixel_size_x)
col_end = int((max_x - extent[0]) / pixel_size_x)
row_start = int((extent[3] - max_y) / pixel_size_y)
row_end = int((extent[3] - min_y) / pixel_size_y)

logger.info(f"Seoul's LCC bounding box: min_x={min_x:.2f}, max_x={max_x:.2f}, min_y={min_y:.2f}, max_y={max_y:.2f}")
logger.info(f"Seoul pixel indices: row_start={row_start}, row_end={row_end}, col_start={col_start}, col_end={col_end}")
logger.info(f"Seoul grid size: {(row_end - row_start)} x {(col_end - col_start)}")

# Verify grid alignment
if (row_end - row_start) != 20 or (col_end - col_start) != 20:
    logger.error(f"Grid size mismatch: expected 20x20, got {(row_end - row_start)}x{(col_end - col_start)}")
    raise ValueError("Grid size mismatch")

# Set up SQLite with extended schema
conn = sqlite3.connect(db_path)
cursor = conn.cursor()

# Drop and recreate the table to ensure correct schema
try:
    cursor.execute("DROP TABLE IF EXISTS ghi_data")
    cursor.execute('''
                   CREATE TABLE ghi_data
                   (
                       timestamp      TEXT,
                       GHI            REAL,
                       DNI            REAL,
                       DHI            REAL,
                       X              REAL,
                       Y              REAL,
                       lat            REAL,
                       lon            REAL,
                       Zenith_Angle   REAL,
                       Solar_Altitude REAL,
                       Solar_Azimuth  REAL,
                       UNIQUE (timestamp, lat, lon)
                   )
                   ''')
    conn.commit()
    logger.info("Recreated ghi_data table with correct schema.")
except Exception as e:
    logger.error(f"Error setting up database: {e}")
    raise


# Function to get the current row count in the database
def get_row_count():
    cursor.execute("SELECT COUNT(*) FROM ghi_data")
    count = cursor.fetchone()[0]
    return count


# Solar geometry functions (consistent with other scripts)
def calculate_zenith_angle(timestamp, latitude, longitude, standard_meridian=135):
    lat_rad = np.radians(latitude)
    day_of_year = timestamp.timetuple().tm_yday
    declination = 23.45 * np.sin(np.radians(360 * (284 + day_of_year) / 365))
    decl_rad = np.radians(declination)
    B = (360 / 365) * (day_of_year - 81)
    EOT = 9.87 * np.sin(np.radians(2 * B)) - 7.53 * np.cos(np.radians(B)) - 1.5 * np.sin(np.radians(B))
    hour = timestamp.hour + timestamp.minute / 60.0
    time_correction = (4 * (longitude - standard_meridian) + EOT) / 60.0
    solar_time = hour + time_correction
    hour_angle = 15 * (solar_time - 12)
    hour_rad = np.radians(hour_angle)
    cos_zenith = (np.sin(lat_rad) * np.sin(decl_rad) +
                  np.cos(lat_rad) * np.cos(decl_rad) * np.cos(hour_rad))
    zenith_angle = np.degrees(np.arccos(np.clip(cos_zenith, -1, 1)))
    return zenith_angle, hour_angle, declination


def calc_solar_altitude(timestamp, latitude, longitude):
    zenith_angle, _, _ = calculate_zenith_angle(timestamp, latitude, longitude)
    solar_altitude = 90 - zenith_angle
    return solar_altitude


def calc_solar_azimuth(zenith, hour_angle, declination, latitude):
    zenith_rad = np.radians(zenith)
    hour_rad = np.radians(hour_angle)
    decl_rad = np.radians(declination)
    lat_rad = np.radians(latitude)
    sin_az = np.sin(hour_rad) * np.cos(decl_rad) / np.sin(zenith_rad)
    cos_az = (np.sin(zenith_rad) * np.sin(lat_rad) - np.sin(decl_rad)) / (np.cos(zenith_rad) * np.cos(lat_rad))
    azimuth = np.degrees(np.arctan2(sin_az, cos_az))
    azimuth = (azimuth + 360) % 360
    return azimuth


# DISC Method for decomposing GHI into DNI and DHI
def disc_method(ghi, zenith_angle, timestamp):
    solar_constant = 1367  # W/m²
    standard_meridian = 135  # KST standard meridian
    day_of_year = timestamp.timetuple().tm_yday
    day_angle = 2 * np.pi / 365 * (day_of_year - 1)
    dec_solar_declination = 0.4093 * np.sin((2 * np.pi / 365) * (day_of_year + 284))
    re = (1.00011 + 0.034221 * np.cos(day_angle) + 0.00128 * np.sin(day_angle) +
          0.000719 * np.cos(2 * day_angle) + 0.000077 * np.sin(2 * day_angle))
    etr = solar_constant * re
    B = np.radians((360 / 365) * (day_of_year - 81))
    eqt = 229.18 * (0.000075 + 0.001868 * np.cos(B) - 0.032077 * np.sin(B) -
                    0.014615 * np.cos(2 * B) - 0.040849 * np.sin(2 * B)) / 60
    cos_zenith = np.cos(np.radians(zenith_angle))
    am = 1 / (cos_zenith + 0.50572 * (96.07995 - zenith_angle) ** -1.6364)
    if zenith_angle >= 90:
        am = np.nan
    i0h = etr * cos_zenith
    if cos_zenith < 0:
        i0h = 0
    kt = ghi / i0h if i0h > 0 else 0.0

    def disc_coefficients(kt):
        if not np.isfinite(kt) or kt < 0 or kt > 1.2:
            return np.nan, np.nan, np.nan
        if kt <= 0.6:
            a = 0.512 - 1.56 * kt + 2.286 * kt ** 2 - 2.222 * kt ** 3
            b = 0.37 + 0.962 * kt
            c = -0.28 + 0.932 * kt - 2.048 * kt ** 2
        else:
            a = -5.743 + 21.77 * kt - 27.49 * kt ** 2 + 11.56 * kt ** 3
            b = 41.40 - 118.5 * kt + 66.05 * kt ** 2 + 31.90 * kt ** 3
            c = -47.01 + 184.2 * kt - 222.0 * kt ** 2 + 73.81 * kt ** 3
        return a, b, c

    a, b, c = disc_coefficients(kt)
    dkn = a + b * np.exp(c * am) if not np.isnan(am) else np.nan
    knc = (0.866 - 0.122 * am + 0.0121 * am ** 2 -
           0.000653 * am ** 3 + 0.000014 * am ** 4) if not np.isnan(am) else np.nan
    dni = (knc - dkn) * etr if not np.isnan(knc) and not np.isnan(dkn) else 0.0
    dni = max(0, dni)
    dhi = ghi - dni * cos_zenith if not np.isnan(dni) else 0.0
    return dni, dhi


# Retry-safe downloader with nearest timestamp fallback
def download_file(original_time_str, search_window_minutes=30):
    file_name = f"gk2a_ami_le2_swrad_ko_{original_time_str}.nc"
    file_path = os.path.join(output_dir, file_name)
    url = f"{base_url}?date={original_time_str}&authKey={auth_key}"

    for attempt in range(5):
        try:
            response = requests.get(url, timeout=30)
            if response.status_code == 200:
                with open(file_path, 'wb') as f:
                    f.write(response.content)
                logger.info(f"Downloaded: {file_name}")
                return file_path, original_time_str
            elif response.status_code == 404:
                logger.warning(f"Attempt {attempt + 1}/5 - File not yet available: {file_name} (404)")
            else:
                logger.warning(
                    f"Attempt {attempt + 1}/5 - Failed to download: {file_name}, Status: {response.status_code}")
        except requests.exceptions.RequestException as e:
            logger.error(f"Attempt {attempt + 1}/5 - Error downloading {file_name}: {str(e)}")

        if attempt < 4:
            time.sleep(5 * (2 ** attempt))

    logger.warning(f"All 5 retries failed for {file_name}. Attempting to fetch nearest available timestamp...")

    base_time = datetime.strptime(original_time_str, "%Y%m%d%H%M")
    search_offsets = [0, -10, 10, -20, 20, -30, 30]
    for offset in search_offsets[1:]:
        nearby_time = base_time + timedelta(minutes=offset)
        nearby_time_str = nearby_time.strftime("%Y%m%d%H%M")
        file_name = f"gk2a_ami_le2_swrad_ko_{nearby_time_str}.nc"
        file_path = os.path.join(output_dir, file_name)
        url = f"{base_url}?date={nearby_time_str}&authKey={auth_key}"

        logger.info(
            f"Attempting to download nearest file: {file_name} (offset {offset} minutes) for original timestamp {original_time_str}")
        for attempt in range(2):
            try:
                response = requests.get(url, timeout=30)
                if response.status_code == 200:
                    with open(file_path, 'wb') as f:
                        f.write(response.content)
                    logger.info(f"Downloaded nearest file: {file_name} for original timestamp {original_time_str}")
                    return file_path, original_time_str
                elif response.status_code == 404:
                    logger.warning(f"Attempt {attempt + 1}/2 - File not yet available: {file_name} (404)")
                else:
                    logger.warning(
                        f"Attempt {attempt + 1}/2 - Failed to download: {file_name}, Status: {response.status_code}")
            except requests.exceptions.RequestException as e:
                logger.error(f"Attempt {attempt + 1}/2 - Error downloading {file_name}: {str(e)}")
            if attempt < 1:
                time.sleep(5 * (2 ** attempt))

    logger.error(
        f"No nearby file found for {original_time_str} within ±{search_window_minutes} minutes. Skipping timestamp.")
    return None, original_time_str


# Process NetCDF file and compute DNI, DHI, and solar geometry
def process_file(file_path, timestamp, actual_time_str):
    try:
        dataset = nc.Dataset(file_path)
        dsr = dataset.variables['DSR'][:]
        dsr = np.flipud(dsr)

        if dsr.shape != (900, 900):
            logger.error(f"Error: Unexpected dimensions {dsr.shape}")
            return []

        data = dsr[row_start:row_end, col_start:col_end]
        if data.shape != (20, 20):
            logger.error(f"Error: Subset not 20x20: {data.shape}")
            return []

        rows_to_insert = []
        timestamp_kst = timestamp + timedelta(hours=9)  # Convert UTC to KST
        for row in range(20):
            for col in range(20):
                full_row = row_start + row
                full_col = col_start + col
                x = extent[0] + full_col * pixel_size_x + pixel_size_x / 2
                y = extent[3] - full_row * pixel_size_y - pixel_size_y / 2
                ghi = float(data[row, col]) if not np.isnan(data[row, col]) else None

                # Use lat, lon directly from locations_df
                idx = row * 20 + col
                lat = locations_df.iloc[idx]['lat']
                lon = locations_df.iloc[idx]['lon']

                if ghi is None or ghi < 0:
                    logger.warning(f"Invalid GHI value {ghi} at row={row}, col={col} in {file_path}. Skipping pixel.")
                    continue

                # Compute solar geometry
                zenith_angle, hour_angle, declination = calculate_zenith_angle(timestamp_kst, lat, lon)
                solar_altitude = calc_solar_altitude(timestamp_kst, lat, lon)
                solar_azimuth = calc_solar_azimuth(zenith_angle, hour_angle, declination, lat)

                # Validate solar geometry parameters
                if not all(np.isfinite([zenith_angle, solar_altitude, solar_azimuth])):
                    logger.warning(
                        f"Invalid solar geometry at row={row}, col={col}: zenith={zenith_angle}, altitude={solar_altitude}, azimuth={solar_azimuth}")
                    continue

                # Compute DNI and DHI using DISC method
                dni, dhi = disc_method(ghi, zenith_angle, timestamp_kst)

                # Validate DNI and DHI
                if not all(np.isfinite([dni, dhi])) or dni < 0 or dhi < 0:
                    logger.warning(f"Invalid DNI/DHI at row={row}, col={col}: DNI={dni}, DHI={dhi}")
                    continue

                rows_to_insert.append((
                    timestamp_kst.strftime("%Y-%m-%d %H:%M:%S"),
                    float(ghi), float(dni), float(dhi),
                    float(x), float(y), float(lat), float(lon),
                    float(zenith_angle), float(solar_altitude), float(solar_azimuth)
                ))
        logger.info(
            f"Processed {os.path.basename(file_path)} (used for timestamp {timestamp_kst.strftime('%Y-%m-%d %H:%M:%S')}): {len(rows_to_insert)} pixels prepared for insertion")
        return rows_to_insert
    except Exception as e:
        logger.error(f"Error processing {file_path}: {str(e)}")
        return []
    finally:
        try:
            dataset.close()
        except:
            pass


# Clean up old data (extended to 24 hours for future fine-tuning)
def cleanup_old_data():
    try:
        cutoff_time = (datetime.now() - timedelta(hours=24)).strftime("%Y-%m-%d %H:%M:%S")
        cursor.execute("DELETE FROM ghi_data WHERE timestamp < ?", (cutoff_time,))
        conn.commit()
        logger.info(f"Cleaned up data older than {cutoff_time}. Rows deleted: {cursor.rowcount}")
    except Exception as e:
        logger.error(f"Error during database cleanup: {str(e)}")


# Fetch and update data every 30 minutes
def fetch_and_update_data():
    while True:
        now = datetime.now().replace(second=0, microsecond=0)
        minute = now.minute
        if minute < 30:
            now_aligned = now.replace(minute=0)
        else:
            now_aligned = now.replace(minute=30)

        # Skip nighttime (20:00 to 03:00 KST)
        current_hour = now.hour
        if current_hour >= 20 or current_hour < 3:
            next_run = (now + timedelta(days=1)).replace(hour=3, minute=0, second=0, microsecond=0)
            if current_hour < 3:
                next_run = now.replace(hour=3, minute=0, second=0, microsecond=0)
            sleep_seconds = (next_run - now).total_seconds()
            logger.info(
                f"Current time {now} is in restricted window (20:00-03:00). Sleeping for {sleep_seconds / 60:.2f} minutes until {next_run}")
            time.sleep(sleep_seconds)
            continue

        end_time = now_aligned - timedelta(minutes=30)
        start_time = end_time - timedelta(hours=4)

        timestamps = pd.date_range(start=start_time, end=end_time, freq='30min')
        time_strings = [t.strftime("%Y%m%d%H%M") for t in timestamps]

        logger.info(f"Fetching data from {start_time} to {end_time} at {now}")
        logger.info(f"Database row count before insert: {get_row_count()}")

        total_entries = 0
        skipped_files = 0
        for time_str, timestamp in zip(time_strings, timestamps):
            timestamp_utc = timestamp - timedelta(hours=9)
            time_str_utc = timestamp_utc.strftime("%Y%m%d%H%M")

            file_path, actual_time_str = download_file(time_str_utc)
            if file_path:
                rows = process_file(file_path, timestamp_utc, actual_time_str)
            else:
                skipped_files += 1
                logger.warning(f"No data available for {timestamp}. Skipping timestamp.")
                continue

            if rows:
                try:
                    cursor.executemany(
                        "INSERT OR REPLACE INTO ghi_data (timestamp, GHI, DNI, DHI, X, Y, lat, lon, Zenith_Angle, Solar_Altitude, Solar_Azimuth) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
                        rows)
                    conn.commit()
                    total_entries += cursor.rowcount
                    logger.info(f"Inserted {cursor.rowcount} new pixels for timestamp {timestamp}")
                    logger.info(f"Database row count after insert: {get_row_count()}")
                except Exception as e:
                    logger.error(f"Error inserting data for {timestamp}: {str(e)}")
            else:
                skipped_files += 1

        logger.info(f"Total number of entries inserted: {total_entries}")
        logger.info(f"Number of skipped files: {skipped_files}")
        logger.info(f"GHI data saved to SQLite DB: {db_path}")

        cleanup_old_data()

        next_update = now_aligned + timedelta(minutes=30)
        sleep_seconds = (next_update - now).total_seconds()
        logger.info(f"Sleeping for {sleep_seconds / 60:.2f} minutes until next update at {next_update}")
        time.sleep(sleep_seconds)


# Main execution
if __name__ == "__main__":
    try:
        fetch_and_update_data()
    except KeyboardInterrupt:
        logger.info("Stopped by user.")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
    finally:
        conn.close()

////////////////////////////////////////////////////...............................................fetch_historical_weather_data.py........................//////////////////////

import requests
import pandas as pd
import sqlite3
from datetime import datetime, timedelta, timezone
import time
import logging
import urllib3
import numpy as np
from scipy.spatial import cKDTree

# Suppress InsecureRequestWarning
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(r"D:\Masters thesis PV\pv_performance_tool\data\fetch_historical_weather_data.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# KMA API setup (historical data)
kma_auth_key = "LoBU7l_-QDuAVO5f_iA7ZQ"
kma_historical_base_url = "https://apihub.kma.go.kr/api/typ01/cgi-bin/url/nph-sfc_obs_nc_pt_api"

# Database setup
db_path = r"D:\Masters thesis PV\pv_performance_tool\data\weather_data.db"
conn = sqlite3.connect(db_path)
cursor = conn.cursor()

# Create table for historical data
try:
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS weather_data (
            timestamp TEXT,
            nx INTEGER,
            ny INTEGER,
            lat REAL,
            lon REAL,
            temperature REAL,
            wind_speed REAL,
            data_type TEXT,
            UNIQUE(timestamp, nx, ny, data_type)
        )
    ''')
    conn.commit()
    logger.info("Ensured weather_data table exists.")
except Exception as e:
    logger.error(f"Error setting up database: {str(e)}")
    raise

# Open-Meteo request tracking
open_meteo_requests = 0
OPEN_METEO_DAILY_LIMIT = 9500  # Cap at 9500 to leave a buffer (limit is 10,000)

def reset_request_counter():
    """Reset the Open-Meteo request counter daily."""
    global open_meteo_requests
    open_meteo_requests = 0
    logger.info("Reset Open-Meteo request counter for the day.")

# Function to get the current row count
def get_row_count(table_name, data_type):
    cursor.execute(f"SELECT COUNT(*) FROM {table_name} WHERE data_type = ?", (data_type,))
    return cursor.fetchone()[0]

# Generate 20x20 grid for Seoul (lat: 37.4–37.7, lon: 126.8–127.2)
coords = [(lat, lon) for lat in np.linspace(37.4, 37.7, 20) for lon in np.linspace(126.8, 127.2, 20)]
locations_df = pd.DataFrame(coords, columns=['lat', 'lon'])
locations_df['nx'] = locations_df.index // 20
locations_df['ny'] = locations_df.index % 20

# KST timezone
KST = timezone(timedelta(hours=9))

# Historical Data Fetching Functions
def fetch_point_data(lat, lon, start_time, end_time, max_retries=3):
    """Fetch historical temperature data for a specific lat/lon point over a time range."""
    tm1 = start_time.strftime("%Y%m%d%H%M")
    tm2 = end_time.strftime("%Y%m%d%H%M")
    params = {
        'obs': 'ta',
        'tm1': tm1,
        'tm2': tm2,
        'itv': '10',
        'lon': lon,
        'lat': lat,
        'authKey': kma_auth_key
    }
    for attempt in range(max_retries):
        try:
            response = requests.get(kma_historical_base_url, params=params, timeout=30, verify=False)
            if response.status_code == 200:
                logger.info(f"Fetched historical data for lat={lat}, lon={lon}, {tm1} to {tm2}")
                logger.debug(f"Response content: {response.text}")
                return response.text
            elif response.status_code == 404:
                logger.warning(f"Attempt {attempt + 1}/{max_retries} - Data not available for lat={lat}, lon={lon}, {tm1} to {tm2} (404)")
                return None
            else:
                logger.warning(f"Attempt {attempt + 1}/{max_retries} - Failed to fetch data for lat={lat}, lon={lon}, {tm1} to {tm2}, Status: {response.status_code}")
        except requests.exceptions.RequestException as e:
            logger.error(f"Attempt {attempt + 1}/{max_retries} - Error fetching data for lat={lat}, lon={lon}, {tm1} to {tm2}: {str(e)}")
        if attempt < max_retries - 1:
            time.sleep(5 * (2 ** attempt))
    logger.error(f"All {max_retries} retries failed for lat={lat}, lon={lon}, {tm1} to {tm2}")
    return None

def parse_point_data(response_text):
    """Parse the point-based API response to extract temperature values at 10-minute intervals."""
    if not response_text:
        return []
    data_points = []
    try:
        lines = response_text.strip().splitlines()
        for line in lines[1:]:
            if not line.strip():
                continue
            parts = line.split(',')
            if len(parts) < 5:
                continue
            timestamp_str = parts[0].strip()
            temp_str = parts[4].strip()
            if temp_str == '-999' or not temp_str:
                continue
            temperature = float(temp_str)
            if not (-20 <= temperature <= 50):
                logger.warning(f"Invalid temperature value {temperature} at timestamp {timestamp_str}")
                continue
            timestamp = pd.to_datetime(timestamp_str, format='%Y%m%d%H%M').tz_localize(KST)
            data_points.append({
                'datetime': timestamp,
                'temperature': temperature
            })
        return data_points
    except Exception as e:
        logger.error(f"Error parsing point data: {str(e)}")
        return []

def fetch_nearest_data(lat, lon, start_time, end_time, all_coords, max_retries=3):
    """Fetch data for the nearest coordinate if the original location has missing data."""
    coords = all_coords[['lat', 'lon']].values
    tree = cKDTree(coords)
    _, idx = tree.query([lat, lon])
    nearest_lat, nearest_lon = coords[idx]
    if nearest_lat == lat and nearest_lon == lon:
        _, indices = tree.query([lat, lon], k=2)
        if len(indices) > 1:
            nearest_lat, nearest_lon = coords[indices[1]]
        else:
            return None
    logger.info(f"Fetching historical data for nearest coordinate (lat={nearest_lat}, lon={nearest_lon}) for time range {start_time} to {end_time}")
    response_text = fetch_point_data(nearest_lat, nearest_lon, start_time, end_time, max_retries)
    if response_text:
        return parse_point_data(response_text)
    return None

def interpolate_to_30min(data_points, start_time, end_time):
    """Interpolate 10-minute interval data to 30-minute intervals."""
    if not data_points:
        return []
    df = pd.DataFrame(data_points)
    df.set_index('datetime', inplace=True)
    if df.index.tz is None:
        df.index = df.index.tz_localize(KST)
    else:
        df.index = df.index.tz_convert(KST)
    time_range = pd.date_range(start=start_time, end=end_time, freq='30min', tz=KST)
    df_30min = df.resample('30min').interpolate(method='linear')
    df_30min = df_30min.reindex(time_range, method='nearest').interpolate(method='linear')
    interpolated_data = []
    for dt, row in df_30min.iterrows():
        interpolated_data.append({
            'datetime': dt.strftime('%Y-%m-%d %H:%M:%S'),
            'temperature': row['temperature']
        })
    return interpolated_data

def interpolate_historical(lat, lon, start_time, end_time, conn, time_window_hours=4):
    """Interpolate missing historical data using stored data for the same location."""
    logger.info(f"Attempting historical interpolation for lat={lat}, lon={lon}, from {start_time} to {end_time}")
    query = """
        SELECT timestamp, temperature
        FROM weather_data
        WHERE lat = ? AND lon = ? AND data_type = 'historical'
        AND timestamp BETWEEN ? AND ?
        ORDER BY timestamp
    """
    try:
        df = pd.read_sql_query(
            query, conn,
            params=(lat, lon,
                    (start_time - timedelta(hours=time_window_hours)).strftime("%Y-%m-%d %H:%M:%S"),
                    (end_time + timedelta(hours=time_window_hours)).strftime("%Y-%m-%d %H:%M:%S"))
        )
        if len(df) < 2:
            logger.warning(f"Insufficient data for historical interpolation at lat={lat}, lon={lon}, from {start_time} to {end_time}")
            return None
        df['timestamp'] = pd.to_datetime(df['timestamp']).dt.tz_localize(KST)
        df.set_index('timestamp', inplace=True)
        time_range = pd.date_range(start=start_time, end=end_time, freq='30min', tz=KST)
        df_30min = df.reindex(time_range, method='nearest').interpolate(method='linear')
        interpolated_data = []
        for dt, row in df_30min.iterrows():
            temperature = row['temperature']
            if not (-20 <= temperature <= 50):
                logger.warning(f"Invalid interpolated temperature value {temperature} at lat={lat}, lon={lon}, timestamp={dt}")
                continue
            interpolated_data.append({
                'datetime': dt.strftime('%Y-%m-%d %H:%M:%S'),
                'temperature': temperature
            })
        logger.info(f"Successfully interpolated historical data for lat={lat}, lon={lon}")
        return interpolated_data
    except Exception as e:
        logger.error(f"Error during historical interpolation: {str(e)}")
        return None

# Open-Meteo Fallback for Historical Data
def fetch_open_meteo_data(locations, start_time, end_time, include_wind_speed=False):
    """Fetch historical data from Open-Meteo for multiple locations."""
    global open_meteo_requests
    all_data_points = []
    for lat, lon in locations:
        if open_meteo_requests >= OPEN_METEO_DAILY_LIMIT:
            logger.warning(f"Approaching Open-Meteo daily limit ({open_meteo_requests}/{OPEN_METEO_DAILY_LIMIT}). Skipping request for lat={lat}, lon={lon}.")
            all_data_points.append((lat, lon, None))
            continue
        url = f"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&hourly=temperature_2m" + (",wind_speed_10m" if include_wind_speed else "")
        url += f"&start_date={start_time.strftime('%Y-%m-%d')}&end_date={end_time.strftime('%Y-%m-%d')}"
        try:
            response = requests.get(url, timeout=30)
            open_meteo_requests += 1
            if response.status_code == 200:
                data = response.json()
                timestamps = pd.to_datetime(data['hourly']['time']).tz_localize(KST)
                temperatures = data['hourly']['temperature_2m']
                winds = data['hourly'].get('wind_speed_10m', [None] * len(timestamps)) if include_wind_speed else [None] * len(timestamps)
                data_points = []
                for ts, temp, wind in zip(timestamps, temperatures, winds):
                    if start_time <= ts <= end_time:
                        if not (-20 <= temp <= 50):
                            logger.warning(f"Invalid temperature value {temp} at lat={lat}, lon={lon}, timestamp={ts}")
                            continue
                        data_point = {'forecast_time': ts, 'temperature': temp}
                        if include_wind_speed:
                            wind_ms = wind / 3.6  # Convert km/h to m/s
                            if not (0 <= wind_ms <= 50):
                                logger.warning(f"Invalid wind speed value {wind_ms} at lat={lat}, lon={lon}, timestamp={ts}")
                                continue
                            data_point['wind_speed'] = wind_ms
                        data_points.append(data_point)
                all_data_points.append((lat, lon, data_points))
                logger.info(f"Fetched Open-Meteo data for lat={lat}, lon={lon} (Request count: {open_meteo_requests})")
            else:
                logger.error(f"Open-Meteo request failed for lat={lat}, lon={lon}: Status {response.status_code}")
                all_data_points.append((lat, lon, None))
        except Exception as e:
            logger.error(f"Open-Meteo error for lat={lat}, lon={lon}: {str(e)}")
            all_data_points.append((lat, lon, None))
        time.sleep(0.1)  # Small delay to avoid rate limiting
    return all_data_points

# Fetch Function for Historical Data
def fetch_historical_data(now):
    total_locations = len(locations_df)
    historical_batch_size = 10

    # Align end_time_hist to the nearest past 30-minute mark
    minutes = (now.minute // 30) * 30
    end_time_hist = now.replace(minute=minutes, second=0, microsecond=0)
    start_time_hist = end_time_hist - timedelta(hours=4)
    historical_timestamps = pd.date_range(start=start_time_hist, end=end_time_hist, freq='30min', tz=KST)
    logger.info(f"Expected historical timestamps: {historical_timestamps}")

    historical_rows = []
    skipped_timestamps = 0

    logger.info(f"Fetching historical data from {start_time_hist} to {end_time_hist}")
    logger.info(f"Historical row count before fetch: {get_row_count('weather_data', 'historical')}")

    for batch_start in range(0, total_locations, historical_batch_size):
        batch_end = min(batch_start + historical_batch_size, total_locations)
        batch_df = locations_df.iloc[batch_start:batch_end]
        logger.info(f"Processing historical batch {batch_start // historical_batch_size + 1}/{(total_locations - 1) // historical_batch_size + 1} (points {batch_start} to {batch_end - 1})")
        for idx, row in batch_df.iterrows():
            lat, lon, nx, ny = row['lat'], row['lon'], row['nx'], row['ny']
            # Fetch from KMA API
            response_text = fetch_point_data(lat, lon, start_time_hist, end_time_hist)
            data_points = parse_point_data(response_text) if response_text else None
            if not data_points:
                # Retry mechanism with buffer
                logger.info(f"Retrying historical fetch for lat={lat}, lon={lon} after 20-sec buffer")
                time.sleep(20)
                response_text = fetch_point_data(lat, lon, start_time_hist, end_time_hist)
                data_points = parse_point_data(response_text) if response_text else None
            if not data_points:
                data_points = fetch_nearest_data(lat, lon, start_time_hist, end_time_hist, locations_df)
            if not data_points:
                interpolated_data = interpolate_historical(lat, lon, start_time_hist, end_time_hist, conn)
                if interpolated_data:
                    for data in interpolated_data:
                        historical_rows.append((
                            data['datetime'],
                            nx, ny, lat, lon,
                            data['temperature'],
                            None,
                            "historical"
                        ))
                    continue
            if not data_points:
                om_data = fetch_open_meteo_data([(lat, lon)], start_time_hist, end_time_hist, include_wind_speed=False)
                if om_data and om_data[0][2]:
                    data_points = [{'datetime': dp['forecast_time'], 'temperature': dp['temperature']} for dp in om_data[0][2]]
                    logger.info(f"Fetched Open-Meteo data points for lat={lat}, lon={lon}: {len(data_points)} timesteps")
                else:
                    logger.warning(f"No Open-Meteo data available for lat={lat}, lon={lon}")
            if not data_points:
                logger.error(f"No historical data available for lat={lat}, lon={lon} from {start_time_hist} to {end_time_hist}")
                skipped_timestamps += len(historical_timestamps)
                continue
            interpolated_data = interpolate_to_30min(data_points, start_time_hist, end_time_hist)
            logger.info(f"Interpolated data for lat={lat}, lon={lon}: {len(interpolated_data)} timesteps")
            for data in interpolated_data:
                historical_rows.append((
                    data['datetime'],
                    nx, ny, lat, lon,
                    data['temperature'],
                    None,
                    "historical"
                ))
        time.sleep(5)

    if historical_rows:
        try:
            cursor.executemany(
                "INSERT OR REPLACE INTO weather_data (timestamp, nx, ny, lat, lon, temperature, wind_speed, data_type) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                historical_rows)
            conn.commit()
            logger.info(f"Inserted {len(historical_rows)} historical weather records")
            logger.info(f"Number of skipped timestamps: {skipped_timestamps}")
            logger.info(f"Historical row count after insert: {get_row_count('weather_data', 'historical')}")
        except Exception as e:
            logger.error(f"Error inserting historical weather data: {str(e)}")

    for _, row in locations_df.iterrows():
        nx, ny = row["nx"], row["ny"]
        cutoff = (now - timedelta(hours=24)).strftime("%Y-%m-%d %H:%M:%S")
        cursor.execute("""
            DELETE FROM weather_data
            WHERE nx = ? AND ny = ? AND timestamp < ? AND data_type = 'historical'
        """, (nx, ny, cutoff))
    conn.commit()
    logger.info(f"Cleaned up historical data, keeping data after {cutoff}")

# Main Fetching Function
def fetch_and_store_historical_weather_data():
    total_locations = len(locations_df)
    last_reset_date = datetime.now(KST).date()

    # Initial fetch for the most recent timestamp
    now = datetime.now(KST).replace(second=0, microsecond=0)
    current_hour = now.hour
    current_minute = now.minute

    # Determine the most recent historical fetch time (:05 or :35)
    if current_minute >= 35:
        last_historical_fetch = now.replace(minute=35, second=0, microsecond=0)
    else:
        last_historical_fetch = now.replace(minute=5, second=0, microsecond=0)
    if current_minute < 5:
        last_historical_fetch -= timedelta(minutes=30)

    # Check if within operating hours (06:00–22:00 KST)
    should_fetch_initial = 6 <= current_hour < 22

    if should_fetch_initial:
        logger.info(f"Performing initial fetch at {now}")
        logger.info(f"Initial historical fetch for timestamp: {last_historical_fetch}")
        fetch_historical_data(last_historical_fetch)
    else:
        logger.info(f"Current time {now} is outside operating hours (06:00–22:00 KST). Skipping initial fetch.")

    # Regular scheduling loop
    while True:
        now = datetime.now(KST).replace(second=0, microsecond=0)
        current_hour = now.hour
        current_minute = now.minute

        # Reset Open-Meteo request counter daily
        current_date = now.date()
        if current_date != last_reset_date:
            reset_request_counter()
            last_reset_date = current_date

        # Operating hours: 06:00–22:00 KST
        if 6 <= current_hour < 22:
            should_fetch_historical = current_minute == 5 or current_minute == 35
            if should_fetch_historical:
                fetch_historical_data(now)

        # Determine the next fetch time
        next_fetch = now.replace(second=0, microsecond=0)
        if current_minute < 5:
            next_fetch = next_fetch.replace(minute=5)
        elif current_minute < 35:
            next_fetch = next_fetch.replace(minute=35)
        else:
            next_fetch = next_fetch.replace(minute=5) + timedelta(hours=1)

        # Skip nighttime (22:00–06:00 KST)
        if next_fetch.hour >= 22:
            next_fetch = next_fetch.replace(hour=6, minute=5) + timedelta(days=1)
        elif next_fetch.hour < 6:
            next_fetch = next_fetch.replace(hour=6, minute=5)

        sleep_seconds = (next_fetch - now).total_seconds()
        if sleep_seconds > 0:
            logger.info(f"Sleeping for {sleep_seconds / 60:.2f} minutes until next fetch at {next_fetch}")
            time.sleep(sleep_seconds)
        else:
            logger.info("No sleep needed, proceeding to next iteration immediately.")

if __name__ == "__main__":
    try:
        fetch_and_store_historical_weather_data()
    except KeyboardInterrupt:
        logger.info("Stopped by user.")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
    finally:
        conn.close()




///////////////////////////////////////////////...............................lnn_forecast.py...................//////////////////////////////////////////

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import sqlite3
from datetime import datetime, timedelta
import logging
import pickle
import time
import os
from scipy.spatial import cKDTree

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(r"D:\Masters thesis PV\pv_performance_tool\data\lnn_forecast.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# LNN Model Definition (same as training)
def get_activation(name):
    if name == "tanh":
        return torch.tanh
    elif name == "relu":
        return torch.nn.functional.relu
    elif name == "leaky_relu":
        return lambda x: torch.nn.functional.leaky_relu(x, negative_slope=0.01)
    elif name == "gelu":
        return torch.nn.functional.gelu
    else:
        raise ValueError(f"Unsupported activation: {name}")

class LiquidTimeStep(nn.Module):
    def __init__(self, input_size, hidden_size, activation_name):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.W_in = nn.Linear(input_size, hidden_size)
        self.W_h = nn.Linear(hidden_size, hidden_size)
        self.tau = nn.Parameter(torch.ones(hidden_size) * 0.1)
        self.activation = get_activation(activation_name)

    def forward(self, x, h, dt=0.1):
        dx = self.activation(self.W_in(x) + self.W_h(h))
        h_new = h + dt * (dx - h) / self.tau
        return h_new

class LiquidNeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers, activation_name, dropout_rate):
        super().__init__()
        self.hidden_size = hidden_size
        self.layers = nn.ModuleList([
            LiquidTimeStep(input_size if i == 0 else hidden_size, hidden_size, activation_name)
            for i in range(num_layers)
        ])
        self.bn = nn.BatchNorm1d(hidden_size)
        self.dropout = nn.Dropout(dropout_rate)
        self.output_layer = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        batch_size, seq_len, _ = x.size()
        h = torch.zeros(batch_size, self.hidden_size, device=x.device)
        for t in range(seq_len):
            h_t = x[:, t, :]
            for layer in self.layers:
                h = layer(h_t, h)
                h_t = h
        h = self.bn(h)
        h = self.dropout(h)
        return self.output_layer(h)

# Load model and scalers
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
prefix_new = 'spatiotemporal_ghi_multistep'
model_save_dir = r"D:\Masters thesis PV\Python codes\PC_2_lnn\trained_models_spatiotemporal_ghi_multistep"
save_dir = r"D:\Masters thesis PV\Python codes\PC_2_lnn\preprocessed_spatiotemporal_ghi"
model_path = f"{model_save_dir}/lnn_3hour_spatiotemporal_ghi_multistep.pth"
scaler_X_path = f"{save_dir}/scaler_X_spatiotemporal_ghi_multistep.pkl"
scaler_y_path = f"{save_dir}/scaler_y_spatiotemporal_ghi_multistep.pkl"
best_params_path = f"{model_save_dir}/best_params_spatiotemporal_ghi_multistep.pkl"
data_info_path = f"{save_dir}/data_info_spatiotemporal_ghi_multistep.txt"

# Load best parameters
with open(best_params_path, "rb") as f:
    best_params = pickle.load(f)

# Load data info
with open(data_info_path, "r") as f:
    data_info = f.readlines()
    timesteps = int(data_info[0].split(": ")[1])
    n_features = int(data_info[1].split(": ")[1])

# Load model with map_location to handle CPU-only machines
lnn_model = LiquidNeuralNetwork(
    input_size=n_features,
    hidden_size=best_params["hidden_size"],
    output_size=6,  # Updated to output 6 GHI values
    num_layers=best_params["num_layers"],
    activation_name=best_params["activation"],
    dropout_rate=best_params["dropout_rate"]
).to(device)
lnn_model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))
lnn_model.eval()

# Load scalers
with open(scaler_X_path, "rb") as f:
    scaler_X = pickle.load(f)
with open(scaler_y_path, "rb") as f:
    scaler_y = pickle.load(f)

# Database paths
weather_db_path = r"D:\Masters thesis PV\pv_performance_tool\data\weather_data.db"
ghi_db_path = r"D:\Masters thesis PV\pv_performance_tool\data\ghi_data.db"
forecast_db_path = r"D:\Masters thesis PV\pv_performance_tool\data\forecast_data.db"

# Connect to databases
conn_weather = sqlite3.connect(weather_db_path)
conn_ghi = sqlite3.connect(ghi_db_path)
conn_forecast = sqlite3.connect(forecast_db_path)
cursor_forecast = conn_forecast.cursor()

# Drop and recreate the forecast_data table to ensure correct schema
cursor_forecast.execute("DROP TABLE IF EXISTS forecast_data")
conn_forecast.commit()
logger.info("Dropped forecast_data table.")

# Create forecast table with explicit REAL type for GHI and add timestep column
cursor_forecast.execute('''
    CREATE TABLE forecast_data (
        timestamp TEXT,
        lat REAL,
        lon REAL,
        GHI REAL,
        timestep INTEGER,
        UNIQUE(timestamp, lat, lon, timestep)
    )
''')
conn_forecast.commit()
logger.info("Created forecast_data table with GHI as REAL and timestep column.")

# Verify the table schema
cursor_forecast.execute("PRAGMA table_info(forecast_data);")
schema = cursor_forecast.fetchall()
logger.info("Forecast_data table schema: %s", schema)

# Define the Seoul coordinates (2-km grid, 20x20 points)
coords = [(lat, lon) for lat in np.linspace(37.4, 37.7, 20) for lon in np.linspace(126.8, 127.2, 20)]
locations_df = pd.DataFrame(coords, columns=['lat', 'lon'])
locations_df = locations_df.drop_duplicates(subset=['lat', 'lon'])
locations_df['nx'] = locations_df.index // 20
locations_df['ny'] = locations_df.index % 20
logger.info(f"locations_df has {len(locations_df)} rows, unique lat/lon pairs: {len(locations_df[['lat', 'lon']].drop_duplicates())}")

# Custom solar geometry functions (as used in training)
def calculate_zenith_angle(timestamp, latitude, longitude, standard_meridian=135):
    """Calculate zenith angle, hour angle, and declination for a given timestamp and location."""
    lat_rad = np.radians(latitude)
    day_of_year = timestamp.timetuple().tm_yday
    declination = 23.45 * np.sin(np.radians(360 * (284 + day_of_year) / 365))
    decl_rad = np.radians(declination)
    B = (360 / 365) * (day_of_year - 81)
    EOT = 9.87 * np.sin(np.radians(2 * B)) - 7.53 * np.cos(np.radians(B)) - 1.5 * np.sin(np.radians(B))
    hour = timestamp.hour + timestamp.minute / 60.0
    time_correction = (4 * (longitude - standard_meridian) + EOT) / 60.0
    solar_time = hour + time_correction
    hour_angle = 15 * (solar_time - 12)
    hour_rad = np.radians(hour_angle)
    cos_zenith = (np.sin(lat_rad) * np.sin(decl_rad) +
                  np.cos(lat_rad) * np.cos(decl_rad) * np.cos(hour_rad))
    zenith_angle = np.degrees(np.arccos(np.clip(cos_zenith, -1, 1)))
    return zenith_angle, hour_angle, declination

def calc_solar_altitude(timestamp, latitude, longitude):
    """Calculate solar altitude from zenith angle."""
    zenith_angle, _, _ = calculate_zenith_angle(timestamp, latitude, longitude)
    solar_altitude = 90 - zenith_angle
    return solar_altitude

def calc_solar_azimuth(zenith, hour_angle, declination, latitude):
    """Calculate solar azimuth using zenith angle, hour angle, declination, and latitude."""
    zenith_rad = np.radians(zenith)
    hour_rad = np.radians(hour_angle)
    decl_rad = np.radians(declination)
    lat_rad = np.radians(latitude)
    sin_az = np.sin(hour_rad) * np.cos(decl_rad) / np.sin(zenith_rad)
    cos_az = (np.sin(zenith_rad) * np.sin(lat_rad) - np.sin(decl_rad)) / (np.cos(zenith_rad) * np.cos(lat_rad))
    azimuth = np.degrees(np.arctan2(sin_az, cos_az))
    azimuth = (azimuth + 360) % 360
    return azimuth

def compute_time_features(timestamp):
    """Compute cyclic time features (day of year, hour)."""
    dt = pd.to_datetime(timestamp)
    day_of_year = dt.timetuple().tm_yday
    hour = dt.hour + dt.minute / 60.0
    day_of_year_sin = np.sin(2 * np.pi * day_of_year / 365.0)
    day_of_year_cos = np.cos(2 * np.pi * day_of_year / 365.0)
    hour_sin = np.sin(2 * np.pi * hour / 24.0)
    hour_cos = np.cos(2 * np.pi * hour / 24.0)
    return day_of_year_sin, day_of_year_cos, hour_sin, hour_cos

def compute_location_features(lat, lon, lat_min, lat_max, lon_min, lon_max):
    """Compute cyclic location features (lat, lon) with the same scaling as training."""
    lat_scaled = (lat - lat_min) / (lat_max - lat_min)
    lon_scaled = (lon - lon_min) / (lon_max - lon_min)
    lat_sin = np.sin(2 * np.pi * lat_scaled)
    lat_cos = np.cos(2 * np.pi * lat_scaled)
    lon_sin = np.sin(2 * np.pi * lon_scaled)
    lon_cos = np.cos(2 * np.pi * lon_scaled)
    return lat_sin, lat_cos, lon_sin, lon_cos

def compute_kt(ghi, solar_altitude):
    """Compute clearness index (Kt)."""
    if solar_altitude <= 0:
        return 0.0
    I0 = 1367  # Solar constant (W/m^2)
    extraterrestrial = I0 * np.sin(np.radians(solar_altitude))
    if extraterrestrial <= 0:
        return 0.0
    kt = ghi / extraterrestrial
    return np.clip(kt, 0.0, 1.2)

def fetch_and_prepare_data(base_time, n_in=8):
    """Fetch and prepare historical data for LNN input."""
    start_time = base_time - timedelta(hours=4)
    end_time = base_time
    timestamps = pd.date_range(start=start_time, end=end_time, freq='30min')
    if len(timestamps) != 9:
        logger.error(f"Expected 9 timesteps, got {len(timestamps)}")
        return None

    query_weather = """
        SELECT timestamp, nx, ny, lat, lon, temperature, wind_speed
        FROM weather_data
        WHERE data_type = 'historical'
        AND timestamp BETWEEN ? AND ?
        ORDER BY nx, ny, timestamp
    """
    df_weather = pd.read_sql_query(
        query_weather,
        conn_weather,
        params=(start_time.strftime("%Y-%m-%d %H:%M:%S"), end_time.strftime("%Y-%m-%d %H:%M:%S"))
    )
    df_weather['timestamp'] = pd.to_datetime(df_weather['timestamp'])
    df_weather = df_weather.drop_duplicates(subset=['timestamp', 'nx', 'ny', 'lat', 'lon'])
    logger.info(f"Fetched weather data: {len(df_weather)} rows")
    logger.info(f"Weather timestamps: {df_weather['timestamp'].unique()}")

    query_ghi = """
        SELECT timestamp, lat, lon, GHI, DNI, DHI
        FROM ghi_data
        WHERE timestamp BETWEEN ? AND ?
        ORDER BY timestamp, lat, lon
    """
    df_ghi = pd.read_sql_query(
        query_ghi,
        conn_ghi,
        params=(start_time.strftime("%Y-%m-%d %H:%M:%S"), end_time.strftime("%Y-%m-%d %H:%M:%S"))
    )
    df_ghi['timestamp'] = pd.to_datetime(df_ghi['timestamp'])
    df_ghi = df_ghi.drop_duplicates(subset=['timestamp', 'lat', 'lon'])
    logger.info(f"Fetched GHI data: {len(df_ghi)} rows")
    logger.info(f"GHI timestamps: {df_ghi['timestamp'].unique()}")

    if df_weather.empty or df_ghi.empty:
        logger.error("No historical data available in the specified time range.")
        return None

    # Find common timestamps
    ghi_timestamps = set(df_ghi['timestamp'])
    weather_timestamps = set(df_weather['timestamp'])
    common_timestamps = ghi_timestamps.intersection(weather_timestamps)
    if not common_timestamps:
        logger.error("No common timestamps between GHI and weather data.")
        return None
    logger.info(f"Common timestamps: {common_timestamps}")

    # Adjust base_time to the latest common timestamp
    latest_common_timestamp = max(common_timestamps)
    adjusted_base_time = pd.to_datetime(latest_common_timestamp)
    time_diff = (base_time - adjusted_base_time).total_seconds() / 3600.0
    if time_diff > 2:  # Don't go back more than 2 hours
        logger.error(f"Adjusted base time {adjusted_base_time} is too far in the past (difference: {time_diff} hours).")
        return None
    start_time = adjusted_base_time - timedelta(hours=4)
    timestamps = pd.date_range(start=start_time, end=adjusted_base_time, freq='30min')
    expected_timesteps = len(timestamps)
    logger.info(f"Adjusted base time to {adjusted_base_time}, new time range: {start_time} to {adjusted_base_time}, expected timesteps: {expected_timesteps}")

    # Filter data to the adjusted time range
    df_weather = df_weather[df_weather['timestamp'].isin(timestamps)]
    df_ghi = df_ghi[df_ghi['timestamp'].isin(timestamps)]
    logger.info(f"Filtered weather data: {len(df_weather)} rows")
    logger.info(f"Filtered GHI data: {len(df_ghi)} rows")

    if df_weather.empty or df_ghi.empty:
        logger.error("No data available in the adjusted time range.")
        return None

    # Map both GHI and weather coordinates to locations_df
    coords_weather = locations_df[['lat', 'lon']].drop_duplicates().values
    tree = cKDTree(coords_weather)

    # Map GHI data
    coords_ghi = df_ghi[['lat', 'lon']].values
    _, idx_ghi = tree.query(coords_ghi)
    df_ghi['lat_mapped'] = locations_df.iloc[idx_ghi]['lat'].values
    df_ghi['lon_mapped'] = locations_df.iloc[idx_ghi]['lon'].values
    df_ghi['nx'] = locations_df.iloc[idx_ghi]['nx'].values
    df_ghi['ny'] = locations_df.iloc[idx_ghi]['ny'].values
    df_ghi = df_ghi.drop_duplicates(subset=['timestamp', 'nx', 'ny'])
    logger.info(f"After mapping, df_ghi has {len(df_ghi)} rows")
    logger.info(f"Unique nx, ny pairs in df_ghi: {len(df_ghi[['nx', 'ny']].drop_duplicates())}")

    # Map weather data
    coords_weather_data = df_weather[['lat', 'lon']].values
    _, idx_weather = tree.query(coords_weather_data)
    df_weather['lat_mapped'] = locations_df.iloc[idx_weather]['lat'].values
    df_weather['lon_mapped'] = locations_df.iloc[idx_weather]['lon'].values
    df_weather['nx'] = locations_df.iloc[idx_weather]['nx'].values
    df_weather['ny'] = locations_df.iloc[idx_weather]['ny'].values
    df_weather = df_weather.drop_duplicates(subset=['timestamp', 'nx', 'ny'])
    logger.info(f"After mapping, df_weather has {len(df_weather)} rows")
    logger.info(f"Unique nx, ny pairs in df_weather: {len(df_weather[['nx', 'ny']].drop_duplicates())}")

    # Merge GHI and weather data on timestamp, nx, ny
    df_merged = pd.merge(
        df_ghi,
        df_weather,
        on=['timestamp', 'nx', 'ny'],
        how='inner'
    )
    df_merged = df_merged.drop_duplicates(subset=['timestamp', 'nx', 'ny'])
    logger.info(f"Merged data: {len(df_merged)} rows")

    if df_merged.empty:
        logger.error("No matching GHI and weather data after merging.")
        return None

    grouped = df_merged.groupby(['nx', 'ny'])
    processed_dfs = []
    lat_min, lat_max = locations_df['lat'].min(), locations_df['lat'].max()
    lon_min, lon_max = locations_df['lon'].min(), locations_df['lon'].max()

    for (nx, ny), group in grouped:
        group = group.sort_values('timestamp')
        lat, lon = group['lat_mapped_x'].iloc[0], group['lon_mapped_x'].iloc[0]

        df_pixel = pd.DataFrame({'timestamp': timestamps})
        df_pixel = df_pixel.merge(group, on='timestamp', how='left')
        actual_timesteps = len(group['timestamp'].unique())
        logger.info(f"Location (nx={nx}, ny={ny}) has {actual_timesteps} unique timesteps, expected {expected_timesteps}")
        if len(df_pixel) != expected_timesteps:
            logger.warning(f"Location (nx={nx}, ny={ny}) has {len(df_pixel)} timesteps after merge, expected {expected_timesteps}.")
            continue
        if actual_timesteps < 5:
            logger.warning(f"Location (nx={nx}, ny={ny}) has too few timesteps ({actual_timesteps}) to forecast.")
            continue

        df_pixel['GHI'] = df_pixel['GHI'].interpolate(method='linear', limit_direction='both')
        df_pixel['DNI'] = df_pixel['DNI'].interpolate(method='linear', limit_direction='both')
        df_pixel['DHI'] = df_pixel['DHI'].interpolate(method='linear', limit_direction='both')
        df_pixel['temperature'] = df_pixel['temperature'].interpolate(method='linear', limit_direction='both')
        df_pixel['lat'] = lat
        df_pixel['lon'] = lon
        df_pixel['nx'] = nx
        df_pixel['ny'] = ny

        df_pixel['Zenith_Angle'], df_pixel['HRA'], df_pixel['DEC'] = zip(*df_pixel['timestamp'].apply(
            lambda ts: calculate_zenith_angle(ts, lat, lon)
        ))
        df_pixel['Solar_Altitude'] = df_pixel['timestamp'].apply(lambda ts: calc_solar_altitude(ts, lat, lon))
        df_pixel['Solar_Azimuth'] = df_pixel.apply(
            lambda row: calc_solar_azimuth(row['Zenith_Angle'], row['HRA'], row['DEC'], lat), axis=1
        )

        df_pixel['Kt'] = df_pixel.apply(lambda row: compute_kt(row['GHI'], row['Solar_Altitude']), axis=1)

        df_pixel[['day_of_year_sin', 'day_of_year_cos', 'hour_sin', 'hour_cos']] = df_pixel['timestamp'].apply(
            lambda ts: pd.Series(compute_time_features(ts))
        ).values
        df_pixel[['lat_sin', 'lat_cos', 'lon_sin', 'lon_cos']] = pd.Series(
            compute_location_features(lat, lon, lat_min, lat_max, lon_min, lon_max)
        ).values.repeat(len(df_pixel)).reshape(-1, 4)

        processed_dfs.append(df_pixel)

    if not processed_dfs:
        logger.error("No locations had sufficient data for forecasting.")
        return None

    return pd.concat(processed_dfs, ignore_index=True)

def forecast_ghi(df, n_forecast=6):
    """Forecast GHI using the LNN model for 6 timesteps."""
    features = [
        'GHI', 'DNI', 'DHI', 'temperature', 'lat_sin', 'lat_cos', 'lon_sin', 'lon_cos',
        'Kt', 'Solar_Altitude', 'Solar_Azimuth', 'day_of_year_sin', 'day_of_year_cos',
        'hour_sin', 'hour_cos'
    ]
    feature_columns = []
    for t in range(-8, 1):  # t-8 to t
        suffix = f"(t-{abs(t)})" if t < 0 else "(t)"
        feature_columns.extend([f"{feat}{suffix}" for feat in features])

    grouped = df.groupby(['nx', 'ny'])
    forecast_dfs = []

    for (nx, ny), group in grouped:
        group = group.sort_values('timestamp')
        lat, lon = group['lat'].iloc[0], group['lon'].iloc[0]

        # Prepare input data
        data_shifted = pd.concat([group[features].shift(i) for i in range(8, -1, -1)], axis=1)
        data_shifted.columns = feature_columns
        data_shifted['timestamp'] = group['timestamp']
        data_shifted['lat'] = lat
        data_shifted['lon'] = lon
        data_shifted = data_shifted.dropna()

        if data_shifted.empty:
            logger.warning(f"No valid data for forecasting at nx={nx}, ny={ny}")
            continue

        # Use the last row for forecasting
        X = data_shifted[feature_columns].values[-1:]  # Shape: (1, 135)
        X_scaled = scaler_X.transform(X)
        X_reshaped = X_scaled.reshape(-1, timesteps, n_features)  # Shape: (1, 9, 15)
        X_tensor = torch.tensor(X_reshaped, dtype=torch.float32).to(device)

        # Forecast GHI for 6 timesteps
        with torch.no_grad():
            forecast_scaled = lnn_model(X_tensor)  # Shape: (1, 6)
        forecast_ghi = np.zeros_like(forecast_scaled.cpu().numpy())  # Shape: (1, 6)
        for i in range(n_forecast):
            forecast_ghi[:, i] = scaler_y.inverse_transform(forecast_scaled[:, i].reshape(-1, 1)).flatten()
        forecast_ghi = forecast_ghi[0]  # Shape: (6,)

        # Generate forecast timestamps
        last_timestamp = pd.Timestamp(data_shifted['timestamp'].iloc[-1])
        forecast_timestamps = [last_timestamp + timedelta(minutes=30 * i) for i in range(1, n_forecast + 1)]

        # Create forecast DataFrame
        forecast_rows = []
        for i, ts in enumerate(forecast_timestamps):
            forecast_rows.append({
                'timestamp': ts,
                'lat': lat,
                'lon': lon,
                'GHI': float(forecast_ghi[i]),
                'timestep': i + 1  # 1 to 6 for t+1 to t+6
            })
        forecast_df = pd.DataFrame(forecast_rows)
        forecast_df['timestamp'] = pd.to_datetime(forecast_df['timestamp'])
        forecast_dfs.append(forecast_df)

    if not forecast_dfs:
        logger.error("No forecasts generated.")
        return None

    return pd.concat(forecast_dfs, ignore_index=True)

def run_forecast_attempt(base_time, n_in, n_forecast):
    try:
        df = fetch_and_prepare_data(base_time, n_in=n_in)
        if df is None:
            return None

        forecast_df = forecast_ghi(df, n_forecast=n_forecast)
        if forecast_df is None:
            logger.error("Failed to generate forecasts.")
            return None

        # Ensure GHI and timestep are appropriate types and prepare forecast rows
        forecast_df['GHI'] = forecast_df['GHI'].astype(float)  # Explicitly convert to float
        forecast_df['timestep'] = forecast_df['timestep'].astype(int)  # Explicitly convert to int
        forecast_rows = [
            (
                pd.Timestamp(row.timestamp).strftime("%Y-%m-%d %H:%M:%S"),
                float(row.lat),
                float(row.lon),
                float(row.GHI),
                int(row.timestep)
            )
            for row in forecast_df[['timestamp', 'lat', 'lon', 'GHI', 'timestep']].to_records(index=False)
        ]

        # Insert into database
        cursor_forecast.executemany(
            "INSERT OR REPLACE INTO forecast_data (timestamp, lat, lon, GHI, timestep) VALUES (?, ?, ?, ?, ?)",
            forecast_rows
        )
        conn_forecast.commit()
        logger.info(f"Inserted {len(forecast_rows)} forecast records into forecast_data.db")

        # Save to CSV, ensuring timestamp is formatted as string
        forecast_df_to_save = forecast_df.copy()
        forecast_df_to_save['timestamp'] = forecast_df_to_save['timestamp'].dt.strftime("%Y-%m-%d %H:%M:%S")
        with open(r"D:\Masters thesis PV\pv_performance_tool\data\forecasted_ghi.csv", 'w') as f:
            pass
        forecast_df_to_save.to_csv(
            r"D:\Masters thesis PV\pv_performance_tool\data\forecasted_ghi.csv",
            index=False
        )
        logger.info("Forecasted GHI data saved to CSV for web integration.")
        return forecast_df

    except Exception as e:
        logger.error(f"Error during forecast for {base_time}: {str(e)}")
        return None

def run_forecast(base_time, n_in=8, n_forecast=6, retries=3, retry_delay=30):
    """Run a single forecast cycle with retries."""
    for attempt in range(retries):
        logger.info(f"Attempt {attempt + 1}/{retries}: Running forecast for {base_time}")
        result = run_forecast_attempt(base_time, n_in, n_forecast)
        if result is not None:
            return result
        logger.info(f"Retrying in {retry_delay/60} minutes...")
        time.sleep(retry_delay)
    logger.error(f"Failed to generate forecast after {retries} attempts.")
    return None

def main_forecast_loop(n_in=8, n_forecast=6, interval_minutes=30):
    """Run forecast loop with updates every interval_minutes (30 or 60)."""
    interval = timedelta(minutes=interval_minutes)

    while True:
        now = datetime.now().replace(second=0, microsecond=0)
        logger.info(f"Checking forecast cycle at {now}")

        # Set base_time to the nearest past 30-minute mark
        minutes = (now.minute // 30) * 30
        base_time = now.replace(minute=minutes)

        # Skip nighttime (20:00 to 03:00 KST)
        current_hour = now.hour
        if current_hour >= 20 or current_hour < 3:
            next_run = (now + timedelta(days=1)).replace(hour=3, minute=0, second=0, microsecond=0)
            if current_hour < 3:
                next_run = now.replace(hour=3, minute=0, second=0, microsecond=0)
            sleep_seconds = (next_run - now).total_seconds()
            logger.info(f"Current time {now} is in restricted window (20:00-03:00). Sleeping for {sleep_seconds / 60:.2f} minutes until {next_run}")
            time.sleep(sleep_seconds)
            continue

        # Run the forecast immediately for the nearest past 30-minute mark
        run_forecast(base_time, n_in, n_forecast, retries=3, retry_delay=300)

        # Calculate the next forecast time
        next_base_time = base_time + interval
        minutes = (next_base_time.minute // 30) * 30
        next_base_time = next_base_time.replace(minute=minutes)

        # Sleep until the next forecast time
        sleep_seconds = (next_base_time - datetime.now()).total_seconds()
        if sleep_seconds > 0:
            logger.info(f"Sleeping for {sleep_seconds / 60:.2f} minutes until next forecast at {next_base_time}")
            time.sleep(sleep_seconds)

if __name__ == "__main__":
    try:
        # Run with 30-minute updates (or set interval_minutes=60 for 1-hour updates)
        main_forecast_loop(interval_minutes=30)
    except KeyboardInterrupt:
        logger.info("Forecast loop stopped by user.")
    finally:
        conn_weather.close()
        conn_ghi.close()
        conn_forecast.close()






////////////////////////............................index.html............................//////////////////////////////////////////////////////////

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PV Performance Prediction Tool</title>
    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome for Icons (used in AdminLTE) -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <!-- Leaflet CSS -->
    <link rel="stylesheet" href="https://unpkg.com/leaflet@1.9.3/dist/leaflet.css" />
    <!-- Plotly JS (Updated to version 2.35.2) -->
    <script src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>
    <!-- Custom CSS -->
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
    <style>
        /* Ensure plots are responsive and fit within the card */
        .card-body .plot-container {
            width: 100%;
            height: 400px;
        }
        .alert-dismissible .close {
            position: absolute;
            top: 0;
            right: 0;
            padding: 0.75rem 1.25rem;
            color: inherit;
        }
    </style>
</head>
<body class="hold-transition sidebar-mini">
<div class="wrapper">
    <!-- Navbar -->
    <nav class="main-header navbar navbar-expand navbar-dark navbar-primary">
        <ul class="navbar-nav">
            <li class="nav-item">
                <a class="nav-link" data-widget="pushmenu" href="#" role="button"><i class="fas fa-bars"></i></a>
            </li>
        </ul>
        <ul class="navbar-nav ml-auto">
            <li class="nav-item">
                <a class="nav-link" href="#">PV Performance Tool</a>
            </li>
        </ul>
    </nav>

    <!-- Sidebar -->
    <aside class="main-sidebar sidebar-dark-primary elevation-4">
        <a href="#" class="brand-link">
            <span class="brand-text font-weight-light">PV Tool</span>
        </a>
        <div class="sidebar">
            <nav class="mt-2">
                <ul class="nav nav-pills nav-sidebar flex-column" data-widget="treeview" role="menu" data-accordion="false">
                    <li class="nav-item">
                        <a href="#" class="nav-link active">
                            <i class="nav-icon fas fa-tachometer-alt"></i>
                            <p>Dashboard</p>
                        </a>
                    </li>
                    <li class="nav-item">
                        <a href="#" class="nav-link">
                            <i class="nav-icon fas fa-info-circle"></i>
                            <p>About</p>
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
    </aside>

    <!-- Content Wrapper -->
    <div class="content-wrapper">
        <!-- Content Header -->
        <div class="content-header">
            <div class="container-fluid">
                <div class="row mb-2">
                    <div class="col-sm-6">
                        <h1 class="m-0">PV Performance Prediction Tool for Seoul</h1>
                    </div>
                </div>
            </div>
        </div>

        <!-- Main Content -->
        <section class="content">
            <div class="container-fluid">
                <!-- Error Message -->
                {% if error %}
                    <div class="alert alert-danger alert-dismissible">
                        <button type="button" class="close" data-dismiss="alert" aria-hidden="true">×</button>
                        <h5><i class="icon fas fa-exclamation-triangle"></i> Error!</h5>
                        {{ error }}
                    </div>
                {% endif %}

                <!-- Forecast Message -->
                {% if forecast_message %}
                    <div class="alert alert-info alert-dismissible">
                        <button type="button" class="close" data-dismiss="alert" aria-hidden="true">×</button>
                        <h5><i class="icon fas fa-info-circle"></i> Forecast Information</h5>
                        {{ forecast_message }}
                    </div>
                {% endif %}

                <!-- Input Form -->
                <div class="card card-primary">
                    <div class="card-header">
                        <h3 class="card-title">Input Parameters</h3>
                    </div>
                    <div class="card-body">
                        <form method="POST" id="pvForm">
                            <!-- Location Selection -->
                            <div class="row mb-3">
                                <div class="col-md-12">
                                    <label class="form-label">Location Selection Method:</label>
                                    <div class="form-check">
                                        <input class="form-check-input" type="radio" name="lat_source" id="mapSelect" value="map" checked>
                                        <label class="form-check-label" for="mapSelect">
                                            Select from Map or Search
                                        </label>
                                    </div>
                                    <div class="form-check">
                                        <input class="form-check-input" type="radio" name="lat_source" id="manualSelect" value="manual">
                                        <label class="form-check-label" for="manualSelect">
                                            Enter Coordinates Manually
                                        </label>
                                    </div>
                                </div>
                            </div>

                            <!-- Map Selection with Search -->
                            <div id="mapSection">
                                <div class="row mb-3">
                                    <div class="col-md-12">
                                        <label for="locationSearch" class="form-label" data-bs-toggle="tooltip" title="Search for a location in Seoul by district (e.g., Jongno-gu, Myeongdong)">Search Location:</label>
                                        <input type="text" id="locationSearch" class="form-control" placeholder="Search by district (e.g., Jongno-gu, Myeongdong)">
                                    </div>
                                </div>
                                <div class="row mb-3">
                                    <div class="col-md-12">
                                        <label for="map" class="form-label" data-bs-toggle="tooltip" title="Click on the map or a marker to select a location in Seoul">Select Location on Map:</label>
                                        <div id="map" style="height: 400px;"></div>
                                    </div>
                                </div>
                                <div class="row mb-3">
                                    <div class="col-md-6">
                                        <label for="latitude" class="form-label">Selected Latitude:</label>
                                        <input type="text" name="latitude" id="latitude" class="form-control" readonly required>
                                    </div>
                                    <div class="col-md-6">
                                        <label for="longitude" class="form-label">Selected Longitude:</label>
                                        <input type="text" name="longitude" id="longitude" class="form-control" readonly required>
                                    </div>
                                </div>
                            </div>

                            <!-- Manual Coordinate Input -->
                            <div id="manualSection" style="display: none;">
                                <div class="row mb-3">
                                    <div class="col-md-6">
                                        <label for="manual_lat" class="form-label" data-bs-toggle="tooltip" title="Enter latitude between 37.4 and 37.7 (Seoul bounds), e.g., 37.4473684210526">Latitude (37.4 to 37.7):</label>
                                        <input type="number" name="manual_lat" id="manual_lat" class="form-control" step="0.0000000000001" min="37.4" max="37.7" placeholder="e.g., 37.4473684210526" required>
                                    </div>
                                    <div class="col-md-6">
                                        <label for="manual_lon" class="form-label" data-bs-toggle="tooltip" title="Enter longitude between 126.8 and 127.2 (Seoul bounds), e.g., 126.926315789474">Longitude (126.8 to 127.2):</label>
                                        <input type="number" name="manual_lon" id="manual_lon" class="form-control" step="0.0000000000001" min="126.8" max="127.2" placeholder="e.g., 126.926315789474" required>
                                    </div>
                                </div>
                            </div>

                            <!-- Other Inputs -->
                            <div class="row">
                                <div class="col-md-6 mb-3">
                                    <label for="pv_type" class="form-label" data-bs-toggle="tooltip" title="Type of PV module affecting efficiency">PV Type:</label>
                                    <select name="pv_type" id="pv_type" class="form-control" required>
                                        {% for pv_type in pv_types %}
                                            <option value="{{ pv_type }}" {% if pv_type == selected_pv_type %}selected{% endif %}>{{ pv_type }}</option>
                                        {% endfor %}
                                    </select>
                                </div>
                                <div class="col-md-6 mb-3">
                                    <label for="area" class="form-label" data-bs-toggle="tooltip" title="Area of the PV panel in square meters">PV Area (m²):</label>
                                    <input type="number" name="area" id="area" class="form-control" value="{{ selected_area if selected_area else 10 }}" step="0.1" min="0" required>
                                </div>
                            </div>
                            <div class="row">
                                <div class="col-md-4 mb-3">
                                    <label for="tilt" class="form-label" data-bs-toggle="tooltip" title="Angle of the PV panel from the horizontal (0-90 degrees)">Tilt (degrees):</label>
                                    <input type="number" name="tilt" id="tilt" class="form-control" value="{{ selected_tilt if selected_tilt else 30 }}" step="0.1" min="0" max="90" required>
                                </div>
                                <div class="col-md-4 mb-3">
                                    <label class="form-label">Orientation Selection Method:</label>
                                    <div class="form-check">
                                        <input class="form-check-input" type="radio" name="orientation_source" id="dropdownOrientation" value="dropdown" checked>
                                        <label class="form-check-label" for="dropdownOrientation">
                                            Select from Dropdown
                                        </label>
                                    </div>
                                    <div class="form-check">
                                        <input class="form-check-input" type="radio" name="orientation_source" id="manualOrientation" value="manual">
                                        <label class="form-check-label" for="manualOrientation">
                                            Enter Manually
                                        </label>
                                    </div>
                                    <div id="dropdownOrientationSection">
                                        <label for="orientation" class="form-label" data-bs-toggle="tooltip" title="Orientation of the PV panel (e.g., 0°=South, -90°=East, 90°=West, 180°=North)">Orientation:</label>
                                        <select name="orientation" id="orientation" class="form-control" required>
                                            {% for value, label in orientation_options %}
                                                <option value="{{ value }}" {% if value == selected_orientation %}selected{% endif %}>{{ label }}</option>
                                            {% endfor %}
                                        </select>
                                    </div>
                                    <div id="manualOrientationSection" style="display: none;">
                                        <label for="manual_orientation" class="form-label" data-bs-toggle="tooltip" title="Enter orientation in degrees (e.g., 0°=South, -90°=East, 90°=West, 180°=North)">Orientation (degrees):</label>
                                        <input type="number" name="manual_orientation" id="manual_orientation" class="form-control" step="0.1" placeholder="e.g., 45">
                                    </div>
                                </div>
                                <div class="col-md-4 mb-3">
                                    <label for="model_type" class="form-label" data-bs-toggle="tooltip" title="SAM model for PV performance calculation">SAM Model:</label>
                                    <select name="model_type" id="model_type" class="form-control" required>
                                        {% for model in model_types %}
                                            <option value="{{ model }}" {% if model == selected_model_type %}selected{% endif %}>{{ model.replace('_', ' ').title() }}</option>
                                        {% endfor %}
                                    </select>
                                </div>
                            </div>
                            <div class="row">
                                <div class="col-md-12 mb-3 d-flex align-items-end">
                                    <button type="submit" class="btn btn-primary w-100">Calculate PV Performance</button>
                                </div>
                            </div>
                        </form>
                    </div>
                </div>

                <!-- Loading Spinner -->
                <div id="loadingSpinner" style="display: none; position: fixed; top: 50%; left: 50%; transform: translate(-50%, -50%);">
                    <div class="spinner-border text-primary" role="status">
                        <span class="visually-hidden">Loading...</span>
                    </div>
                </div>

                <!-- Current Results -->
                {% if current_results %}
                    <div class="card card-info">
                        <div class="card-header">
                            <h3 class="card-title">Current PV Performance</h3>
                        </div>
                        <div class="card-body">
                            <div class="table-responsive">
                                <table class="table table-striped">
                                    <thead>
                                        <tr>
                                            <th>Timestamp</th>
                                            <th>GHI (W/m²)</th>
                                            <th>GTI (W/m²)</th>
                                            <th>Pdc (W)</th>
                                            <th>Pac (W)</th>
                                            <th>I (A)</th>
                                            <th>V (V)</th>
                                            <th>Temperature (°C)</th>
                                            <th>Wind Speed (m/s)</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td>{{ current_results.timestamp }}</td>
                                            <td>{{ current_results.ghi }}</td>
                                            <td>{{ current_results.gti }}</td>
                                            <td>{{ current_results.pdc }}</td>
                                            <td>{{ current_results.pac }}</td>
                                            <td>{{ current_results.I }}</td>
                                            <td>{{ current_results.V }}</td>
                                            <td>{{ current_results.temperature }}</td>
                                            <td>{{ current_results.wind_speed }}</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>
                    </div>
                {% endif %}

                <!-- Forecast Results -->
                {% if forecast_results %}
                    <div class="card card-success">
                        <div class="card-header">
                            <h3 class="card-title">Forecasted PV Performance</h3>
                        </div>
                        <div class="card-body">
                            <div class="table-responsive">
                                <table class="table table-striped">
                                    <thead>
                                        <tr>
                                            <th>Timestamp</th>
                                            <th>GHI (W/m²)</th>
                                            <th>GTI (W/m²)</th>
                                            <th>Pdc (W)</th>
                                            <th>Pac (W)</th>
                                            <th>I (A)</th>
                                            <th>V (V)</th>
                                            <th>Temperature (°C)</th>
                                            <th>Wind Speed (m/s)</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        {% for result in forecast_results %}
                                            <tr>
                                                <td>{{ result.timestamp }}</td>
                                                <td>{{ result.ghi }}</td>
                                                <td>{{ result.gti }}</td>
                                                <td>{{ result.pdc }}</td>
                                                <td>{{ result.pac }}</td>
                                                <td>{{ result.I }}</td>
                                                <td>{{ result.V }}</td>
                                                <td>{{ result.temperature }}</td>
                                                <td>{{ result.wind_speed }}</td>
                                            </tr>
                                        {% endfor %}
                                    </tbody>
                                </table>
                            </div>
                        </div>
                    </div>

                    <!-- IEC Metrics -->
                    <div class="card card-warning">
                        <div class="card-header">
                            <h3 class="card-title">IEC Performance Metrics</h3>
                        </div>
                        <div class="card-body">
                            <div class="table-responsive">
                                <table class="table table-bordered">
                                    <thead>
                                        <tr>
                                            <th>Metric</th>
                                            <th>Value</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        {% for key, value in iec_metrics.items() %}
                                            <tr>
                                                <td>{{ key }}</td>
                                                <td>{{ value }}</td>
                                            </tr>
                                        {% endfor %}
                                    </tbody>
                                </table>
                            </div>
                        </div>
                    </div>

                    <!-- Plots -->
                    <div class="row">
                        <div class="col-md-6 mb-4">
                            <div class="card card-info">
                                <div class="card-header">
                                    <h3 class="card-title">GHI (Current vs Forecasted)</h3>
                                </div>
                                <div class="card-body">
                                    <div id="ghi_plot" class="plot-container"></div>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-6 mb-4">
                            <div class="card card-info">
                                <div class="card-header">
                                    <h3 class="card-title">PV Power Output (Pac) (Current vs Forecasted)</h3>
                                </div>
                                <div class="card-body">
                                    <div id="pac_plot" class="plot-container"></div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="row">
                        <div class="col-md-12 mb-4">
                            <div class="card card-info">
                                <div class="card-header">
                                    <h3 class="card-title">Current and Voltage at MPP (Current vs Forecasted)</h3>
                                </div>
                                <div class="card-body">
                                    <div id="iv_plot" class="plot-container"></div>
                                </div>
                            </div>
                        </div>
                    </div>
                {% endif %}
            </div>
        </section>
    </div>

    <!-- Footer -->
    <footer class="main-footer">
        <div class="float-right d-none d-sm-block">
            <b>Version</b> 1.0.0
        </div>
        <strong>© 2025 PV Performance Prediction Tool. All rights reserved.</strong>
    </footer>
</div>

<!-- Leaflet JS -->
<script src="https://unpkg.com/leaflet@1.9.3/dist/leaflet.js"></script>
<!-- Bootstrap 5 JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<!-- Custom JS -->
<script src="{{ url_for('static', filename='js/scripts.js') }}"></script>

<!-- Define locations variable -->
<script>
    var locations = {{ location_options_with_labels | tojson | default('[]') }};
    console.log("Locations data:", locations);
</script>

<!-- Main JavaScript -->
<script>
document.addEventListener('DOMContentLoaded', function() {
    // Map Initialization
    try {
        console.log('Map script loaded successfully');

        // Verify Leaflet library loaded
        if (typeof L === 'undefined') {
            throw new Error("Leaflet library failed to load.");
        }

        // Verify map element exists
        var mapElement = document.getElementById('map');
        if (!mapElement) {
            throw new Error("Map element with id='map' not found in the DOM.");
        }

        // Define icon configurations as separate variables
        var defaultIcon = L.icon({
            iconUrl: 'https://unpkg.com/leaflet@1.9.3/dist/images/marker-icon.png',
            iconSize: [25, 41],
            iconAnchor: [12, 41]
        });

        var selectedIcon = L.icon({
            iconUrl: 'https://raw.githubusercontent.com/pointhi/leaflet-color-markers/master/img/marker-icon-red.png',
            iconSize: [25, 41],
            iconAnchor: [12, 41]
        });

        var customIcon = L.icon({
            iconUrl: 'https://raw.githubusercontent.com/pointhi/leaflet-color-markers/master/img/marker-icon-blue.png',
            iconSize: [25, 41],
            iconAnchor: [12, 41]
        });

        // Initialize the map centered on Seoul
        var map = L.map('map').setView([37.55, 127.0], 12);

        // Add OpenStreetMap tiles
        L.tileLayer('https://tile.openstreetmap.org/{z}/{x}/{y}.png', {
            attribution: '© <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
        }).addTo(map);

        // Add markers for the locations from locations_eng.xlsx
        var selectedMarker = null;
        var markers = [];
        var customMarker = null;

        function updateMap(filteredLocations) {
            // Clear existing markers except the custom marker
            markers.forEach(marker => map.removeLayer(marker));
            markers = [];

            // Add new markers
            filteredLocations.forEach(function(loc) {
                if (!Array.isArray(loc) || loc.length < 3) {
                    console.error("Invalid location data:", loc);
                    return;
                }
                var marker = L.marker([loc[0], loc[1]], { icon: defaultIcon }).addTo(map);
                marker.bindPopup(loc[2]);
                marker.on('click', function() {
                    document.getElementById('latitude').value = loc[0];
                    document.getElementById('longitude').value = loc[1];

                    if (selectedMarker) {
                        selectedMarker.setIcon(defaultIcon);
                    }
                    marker.setIcon(selectedIcon);
                    selectedMarker = marker;

                    // Remove custom marker if it exists
                    if (customMarker) {
                        map.removeLayer(customMarker);
                        customMarker = null;
                    }
                });
                markers.push(marker);
            });
        }

        // Initial map population
        if (locations && locations.length > 0) {
            updateMap(locations);
            // Set default selection to the first location
            var latInput = document.getElementById('latitude');
            var lonInput = document.getElementById('longitude');
            if (latInput && lonInput) {
                latInput.value = locations[0][0];
                lonInput.value = locations[0][1];
                markers[0].setIcon(selectedIcon);
                selectedMarker = markers[0];
            } else {
                console.error("Latitude or longitude input elements not found.");
            }
        } else {
            console.error("No locations available to display on the map.");
        }

        // Search functionality
        var locationSearch = document.getElementById('locationSearch');
        if (locationSearch) {
            locationSearch.addEventListener('input', function(e) {
                var searchValue = e.target.value.toLowerCase();
                var filteredLocations = locations.filter(function(loc) {
                    return loc[2].toLowerCase().includes(searchValue);
                });
                updateMap(filteredLocations);
            });
        } else {
            console.error("Location search input element not found.");
        }

        // Map click event to select a custom location
        map.on('click', function(e) {
            var lat = e.latlng.lat;
            var lng = e.latlng.lng;

            // Validate coordinates within Seoul bounds
            if (lat < 37.4 || lat > 37.7 || lng < 126.8 || lng > 127.2) {
                alert("Selected location is outside Seoul bounds (Lat: 37.4 to 37.7, Lon: 126.8 to 127.2). Please select a location within Seoul.");
                return;
            }

            // Update form fields
            var latInput = document.getElementById('latitude');
            var lonInput = document.getElementById('longitude');
            if (latInput && lonInput) {
                latInput.value = lat;
                lonInput.value = lng;
            } else {
                console.error("Latitude or longitude input elements not found.");
                return;
            }

            // Remove existing custom marker if it exists
            if (customMarker) {
                map.removeLayer(customMarker);
            }

            // Add a new custom marker at the clicked location
            customMarker = L.marker([lat, lng], { icon: customIcon }).addTo(map);
            customMarker.bindPopup('Custom Location: Lat: ' + lat.toFixed(6) + ', Lon: ' + lng.toFixed(6)).openPopup();

            // Deselect any previously selected marker
            if (selectedMarker) {
                selectedMarker.setIcon(defaultIcon);
                selectedMarker = null;
            }
        });
    } catch (error) {
        console.error("Error initializing map:", error);
    }

    // Form Event Handlers and Validation
    try {
        // Toggle between map and manual input
        var mapSelect = document.getElementById('mapSelect');
        var manualSelect = document.getElementById('manualSelect');
        if (mapSelect && manualSelect) {
            mapSelect.addEventListener('change', function() {
                document.getElementById('mapSection').style.display = 'block';
                document.getElementById('manualSection').style.display = 'none';
                document.getElementById('latitude').setAttribute('required', 'required');
                document.getElementById('longitude').setAttribute('required', 'required');
                document.getElementById('manual_lat').removeAttribute('required');
                document.getElementById('manual_lon').removeAttribute('required');
            });

            manualSelect.addEventListener('change', function() {
                document.getElementById('mapSection').style.display = 'none';
                document.getElementById('manualSection').style.display = 'block';
                document.getElementById('latitude').removeAttribute('required');
                document.getElementById('longitude').removeAttribute('required');
                document.getElementById('manual_lat').setAttribute('required', 'required');
                document.getElementById('manual_lon').setAttribute('required', 'required');
            });
        } else {
            console.error("Map or manual select radio buttons not found.");
        }

        // Toggle between dropdown and manual orientation
        var dropdownOrientation = document.getElementById('dropdownOrientation');
        var manualOrientation = document.getElementById('manualOrientation');
        if (dropdownOrientation && manualOrientation) {
            dropdownOrientation.addEventListener('change', function() {
                document.getElementById('dropdownOrientationSection').style.display = 'block';
                document.getElementById('manualOrientationSection').style.display = 'none';
                document.getElementById('orientation').setAttribute('required', 'required');
                document.getElementById('manual_orientation').removeAttribute('required');
            });

            manualOrientation.addEventListener('change', function() {
                document.getElementById('dropdownOrientationSection').style.display = 'none';
                document.getElementById('manualOrientationSection').style.display = 'block';
                document.getElementById('orientation').removeAttribute('required');
                document.getElementById('manual_orientation').setAttribute('required', 'required');
            });
        } else {
            console.error("Orientation radio buttons not found.");
        }

        // Client-side form validation
        var pvForm = document.getElementById('pvForm');
        if (pvForm) {
            pvForm.addEventListener('submit', function(event) {
                var latSource = document.querySelector('input[name="lat_source"]:checked');
                if (!latSource) {
                    alert("Please select a location selection method.");
                    event.preventDefault();
                    return;
                }
                latSource = latSource.value;
                if (latSource === 'map') {
                    var lat = document.getElementById('latitude').value;
                    var lon = document.getElementById('longitude').value;
                    if (!lat || !lon) {
                        alert("Please select a location on the map or via search before submitting.");
                        event.preventDefault();
                        return;
                    }
                    var latNum = parseFloat(lat);
                    var lonNum = parseFloat(lon);
                    if (latNum < 37.4 || latNum > 37.7 || lonNum < 126.8 || lonNum > 127.2) {
                        alert("Selected location is outside Seoul bounds (Lat: 37.4 to 37.7, Lon: 126.8 to 127.2). Please select a location within Seoul.");
                        event.preventDefault();
                        return;
                    }
                } else if (latSource === 'manual') {
                    var manualLat = document.getElementById('manual_lat').value;
                    var manualLon = document.getElementById('manual_lon').value;
                    if (!manualLat || !manualLon) {
                        alert("Please enter valid latitude and longitude values.");
                        event.preventDefault();
                        return;
                    }
                    var latNum = parseFloat(manualLat);
                    var lonNum = parseFloat(manualLon);
                    if (latNum < 37.4 || latNum > 37.7 || lonNum < 126.8 || lonNum > 127.2) {
                        alert("Latitude must be between 37.4 and 37.7, and longitude must be between 126.8 and 127.2.");
                        event.preventDefault();
                        return;
                    }
                }
            });
        } else {
            console.error("Form element with id='pvForm' not found.");
        }
    } catch (error) {
        console.error("Error initializing event handlers:", error);
    }

    // Plotly Charts
    {% if current_results and forecast_results %}
        try {
            // Validate Plotly library
            if (typeof Plotly === 'undefined') {
                throw new Error("Plotly library failed to load.");
            }

            // GHI Plot
            var current_timestamp = {{ current_timestamp | tojson | default('""') }};
            var current_ghi = {{ current_ghi | tojson | default(0) }};
            var timestamps = {{ timestamps | tojson | default('[]') }};
            var ghi = {{ ghi | tojson | default('[]') }};

            // Validate data
            if (!current_timestamp || typeof current_ghi !== 'number') {
                console.error("Invalid current GHI data:", { current_timestamp, current_ghi });
                current_timestamp = "";
                current_ghi = 0;
            }
            if (!Array.isArray(timestamps) || !Array.isArray(ghi) || timestamps.length !== ghi.length) {
                console.error("Invalid forecast GHI data:", { timestamps, ghi });
                timestamps = [];
                ghi = [];
            }

            var trace1 = {
                x: [current_timestamp],
                y: [current_ghi],
                type: 'scatter',
                mode: 'markers',
                name: 'Current GHI (W/m²)',
                marker: { color: '#FF5733', size: 10 }
            };
            var trace2 = {
                x: timestamps,
                y: ghi,
                type: 'scatter',
                mode: 'lines+markers',
                name: 'Forecasted GHI (W/m²)',
                line: { color: '#FF5733', dash: 'dash' }
            };

            var layout1 = {
                title: 'GHI (Current vs Forecasted)',
                xaxis: { 
                    title: 'Timestamp',
                    tickangle: 45,
                    automargin: true
                },
                yaxis: { 
                    title: 'GHI (W/m²)',
                    rangemode: 'tozero'
                },
                legend: { x: 0, y: 1.2 },
                showlegend: true,
                margin: { t: 50, b: 100 },
                responsive: true
            };

            Plotly.newPlot('ghi_plot', [trace1, trace2], layout1);

            // Pac Plot
            var current_pac = {{ current_pac | tojson | default(0) }};
            var pac = {{ pac | tojson | default('[]') }};

            // Validate data
            if (typeof current_pac !== 'number') {
                console.error("Invalid current Pac data:", current_pac);
                current_pac = 0;
            }
            if (!Array.isArray(pac)) {
                console.error("Invalid forecast Pac data:", pac);
                pac = [];
            }

            var trace3 = {
                x: [current_timestamp],
                y: [current_pac],
                type: 'scatter',
                mode: 'markers',
                name: 'Current Pac (W)',
                marker: { color: '#3357FF', size: 10 }
            };
            var trace4 = {
                x: timestamps,
                y: pac,
                type: 'scatter',
                mode: 'lines+markers',
                name: 'Forecasted Pac (W)',
                line: { color: '#3357FF', dash: 'dash' }
            };

            var layout2 = {
                title: 'PV Power Output (Pac) (Current vs Forecasted)',
                xaxis: { 
                    title: 'Timestamp',
                    tickangle: 45,
                    automargin: true
                },
                yaxis: { 
                    title: 'Pac (W)',
                    rangemode: 'tozero'
                },
                legend: { x: 0, y: 1.2 },
                showlegend: true,
                margin: { t: 50, b: 100 },
                responsive: true
            };

            Plotly.newPlot('pac_plot', [trace3, trace4], layout2);

            // I-V Plot
            var current_I = {{ current_I | tojson | default(0) }};
            var current_V = {{ current_V | tojson | default(0) }};
            var I = {{ I | tojson | default('[]') }};
            var V = {{ V | tojson | default('[]') }};

            // Validate data
            if (typeof current_I !== 'number' || typeof current_V !== 'number') {
                console.error("Invalid current I/V data:", { current_I, current_V });
                current_I = 0;
                current_V = 0;
            }
            if (!Array.isArray(I) || !Array.isArray(V)) {
                console.error("Invalid forecast I/V data:", { I, V });
                I = [];
                V = [];
            }

            var trace5 = {
                x: [current_timestamp],
                y: [current_I],
                type: 'scatter',
                mode: 'markers',
                name: 'Current I (A)',
                marker: { color: '#FF33A1', size: 10 }
            };
            var trace6 = {
                x: [current_timestamp],
                y: [current_V],
                type: 'scatter',
                mode: 'markers',
                name: 'Current V (V)',
                marker: { color: '#33FFF5', size: 10 }
            };
            var trace7 = {
                x: timestamps,
                y: I,
                type: 'scatter',
                mode: 'lines+markers',
                name: 'Forecasted I (A)',
                line: { color: '#FF33A1', dash: 'dash' }
            };
            var trace8 = {
                x: timestamps,
                y: V,
                type: 'scatter',
                mode: 'lines+markers',
                name: 'Forecasted V (V)',
                line: { color: '#33FFF5', dash: 'dash' }
            };

            var layout3 = {
                title: 'Current and Voltage at MPP (Current vs Forecasted)',
                xaxis: { 
                    title: 'Timestamp',
                    tickangle: 45,
                    automargin: true
                },
                yaxis: { 
                    title: 'Value',
                    rangemode: 'tozero'
                },
                legend: { x: 0, y: 1.2 },
                showlegend: true,
                margin: { t: 50, b: 100 },
                responsive: true
            };

            Plotly.newPlot('iv_plot', [trace5, trace6, trace7, trace8], layout3);

            // Loading Spinner
            document.getElementById('pvForm').addEventListener('submit', function() {
                document.getElementById('loadingSpinner').style.display = 'block';
            });

            // Enable Bootstrap Tooltips
            var tooltipTriggerList = [].slice.call(document.querySelectorAll('[data-bs-toggle="tooltip"]'));
            var tooltipList = tooltipTriggerList.map(function(tooltipTriggerEl) {
                return new bootstrap.Tooltip(tooltipTriggerEl);
            });
        } catch (error) {
            console.error("Error initializing charts:", error);
            // Display a user-friendly message if plots fail to render
            document.getElementById('ghi_plot').innerHTML = "<p class='text-danger'>Error rendering GHI plot. Please check the console for details.</p>";
            document.getElementById('pac_plot').innerHTML = "<p class='text-danger'>Error rendering Pac plot. Please check the console for details.</p>";
            document.getElementById('iv_plot').innerHTML = "<p class='text-danger'>Error rendering I-V plot. Please check the console for details.</p>";
        }
    {% endif %}
});
</script>
</body>
</html>